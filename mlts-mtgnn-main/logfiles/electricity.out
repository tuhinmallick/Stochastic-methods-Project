Namespace(L1Loss=True, batch_size=4, buildA_true=True, clip=5, conv_channels=16, data='./data/electricity.txt', device='cuda:0', dilation_exponential=2, dropout=0.3, end_channels=64, epochs=30, gcn_depth=2, gcn_true=True, horizon=3, in_dim=1, layers=5, log_interval=2000, lr=0.0001, node_dim=40, normalize=2, num_nodes=321, num_split=1, optim='adam', propalpha=0.05, residual_channels=16, save='./models/model-electricity-3.pt', seq_in_len=168, seq_out_len=1, skip_channels=32, step_size=100, subgraph_size=20, tanhalpha=3, weight_decay=1e-05)
The recpetive field size is 187
Number of model parameters is 362385
begin training
iter:  0 | loss: 1860.391
iter:100 | loss: 424.127
iter:200 | loss: 466.066
iter:300 | loss: 381.595
iter:400 | loss: 215.805
iter:500 | loss: 262.050
iter:600 | loss: 327.973
iter:700 | loss: 369.448
iter:800 | loss: 210.651
iter:900 | loss: 263.337
iter:1000 | loss: 427.350
iter:1100 | loss: 212.097
iter:1200 | loss: 257.929
iter:1300 | loss: 153.985
iter:1400 | loss: 181.425
iter:1500 | loss: 286.315
iter:1600 | loss: 191.895
iter:1700 | loss: 151.014
iter:1800 | loss: 189.318
iter:1900 | loss: 266.137
iter:2000 | loss: 158.651
iter:2100 | loss: 270.315
iter:2200 | loss: 210.450
iter:2300 | loss: 211.279
iter:2400 | loss: 190.530
iter:2500 | loss: 176.365
iter:2600 | loss: 179.319
iter:2700 | loss: 190.562
iter:2800 | loss: 235.207
iter:2900 | loss: 185.838
iter:3000 | loss: 238.726
iter:3100 | loss: 160.001
iter:3200 | loss: 188.269
iter:3300 | loss: 193.547
iter:3400 | loss: 157.881
iter:3500 | loss: 194.815
iter:3600 | loss: 139.135
iter:3700 | loss: 265.037
iter:3800 | loss: 155.628
iter:3900 | loss: 264.601
| end of epoch   1 | time: 850.74s | train_loss 247.7759 | valid rse 0.0589 | valid rae 0.0424 | valid corr  0.9214
iter:  0 | loss: 195.302
iter:100 | loss: 134.885
iter:200 | loss: 267.379
iter:300 | loss: 254.704
iter:400 | loss: 212.722
iter:500 | loss: 219.508
iter:600 | loss: 131.462
iter:700 | loss: 209.033
iter:800 | loss: 131.104
iter:900 | loss: 172.938
iter:1000 | loss: 171.509
iter:1100 | loss: 145.708
iter:1200 | loss: 159.032
iter:1300 | loss: 184.975
iter:1400 | loss: 272.820
iter:1500 | loss: 193.164
iter:1600 | loss: 234.782
iter:1700 | loss: 255.095
iter:1800 | loss: 139.076
iter:1900 | loss: 108.292
iter:2000 | loss: 152.906
iter:2100 | loss: 171.185
iter:2200 | loss: 201.103
iter:2300 | loss: 193.655
iter:2400 | loss: 173.071
iter:2500 | loss: 196.810
iter:2600 | loss: 145.990
iter:2700 | loss: 216.102
iter:2800 | loss: 191.446
iter:2900 | loss: 244.874
iter:3000 | loss: 170.316
iter:3100 | loss: 228.616
iter:3200 | loss: 153.996
iter:3300 | loss: 235.132
iter:3400 | loss: 166.407
iter:3500 | loss: 196.735
iter:3600 | loss: 158.692
iter:3700 | loss: 131.026
iter:3800 | loss: 121.465
iter:3900 | loss: 126.200
| end of epoch   2 | time: 855.09s | train_loss 178.6162 | valid rse 0.0574 | valid rae 0.0408 | valid corr  0.9281
iter:  0 | loss: 154.290
iter:100 | loss: 134.599
iter:200 | loss: 129.508
iter:300 | loss: 109.912
iter:400 | loss: 145.520
iter:500 | loss: 191.517
iter:600 | loss: 143.099
iter:700 | loss: 258.624
iter:800 | loss: 152.983
iter:900 | loss: 165.648
iter:1000 | loss: 154.682
iter:1100 | loss: 184.302
iter:1200 | loss: 132.140
iter:1300 | loss: 235.266
iter:1400 | loss: 131.029
iter:1500 | loss: 118.381
iter:1600 | loss: 165.662
iter:1700 | loss: 161.512
iter:1800 | loss: 180.133
iter:1900 | loss: 279.414
iter:2000 | loss: 131.449
iter:2100 | loss: 126.115
iter:2200 | loss: 133.317
iter:2300 | loss: 95.301
iter:2400 | loss: 144.412
iter:2500 | loss: 179.932
iter:2600 | loss: 136.269
iter:2700 | loss: 194.736
iter:2800 | loss: 227.081
iter:2900 | loss: 132.424
iter:3000 | loss: 152.241
iter:3100 | loss: 167.886
iter:3200 | loss: 162.749
iter:3300 | loss: 168.339
iter:3400 | loss: 112.273
iter:3500 | loss: 207.193
iter:3600 | loss: 158.063
iter:3700 | loss: 160.721
iter:3800 | loss: 306.395
iter:3900 | loss: 162.735
| end of epoch   3 | time: 856.11s | train_loss 169.1093 | valid rse 0.0586 | valid rae 0.0417 | valid corr  0.9300
iter:  0 | loss: 187.758
iter:100 | loss: 168.059
iter:200 | loss: 175.144
iter:300 | loss: 179.839
iter:400 | loss: 187.383
iter:500 | loss: 146.596
iter:600 | loss: 228.619
iter:700 | loss: 238.411
iter:800 | loss: 162.816
iter:900 | loss: 158.638
iter:1000 | loss: 161.271
iter:1100 | loss: 110.197
iter:1200 | loss: 221.057
iter:1300 | loss: 182.511
iter:1400 | loss: 152.755
iter:1500 | loss: 118.929
iter:1600 | loss: 115.962
iter:1700 | loss: 277.931
iter:1800 | loss: 117.049
iter:1900 | loss: 104.388
iter:2000 | loss: 178.629
iter:2100 | loss: 116.839
iter:2200 | loss: 130.258
iter:2300 | loss: 159.077
iter:2400 | loss: 120.985
iter:2500 | loss: 120.714
iter:2600 | loss: 129.640
iter:2700 | loss: 179.681
iter:2800 | loss: 194.187
iter:2900 | loss: 89.899
iter:3000 | loss: 173.315
iter:3100 | loss: 181.469
iter:3200 | loss: 151.002
iter:3300 | loss: 192.459
iter:3400 | loss: 134.942
iter:3500 | loss: 185.815
iter:3600 | loss: 106.271
iter:3700 | loss: 133.842
iter:3800 | loss: 121.811
iter:3900 | loss: 143.806
| end of epoch   4 | time: 856.59s | train_loss 165.0569 | valid rse 0.0544 | valid rae 0.0387 | valid corr  0.9316
iter:  0 | loss: 187.900
iter:100 | loss: 259.430
iter:200 | loss: 297.874
iter:300 | loss: 249.719
iter:400 | loss: 131.766
iter:500 | loss: 153.003
iter:600 | loss: 174.018
iter:700 | loss: 139.121
iter:800 | loss: 188.763
iter:900 | loss: 117.824
iter:1000 | loss: 200.765
iter:1100 | loss: 176.033
iter:1200 | loss: 128.973
iter:1300 | loss: 191.634
iter:1400 | loss: 139.560
iter:1500 | loss: 184.666
iter:1600 | loss: 225.784
iter:1700 | loss: 202.692
iter:1800 | loss: 109.833
iter:1900 | loss: 258.909
iter:2000 | loss: 180.482
iter:2100 | loss: 172.947
iter:2200 | loss: 174.609
iter:2300 | loss: 183.309
iter:2400 | loss: 139.761
iter:2500 | loss: 126.210
iter:2600 | loss: 236.913
iter:2700 | loss: 123.119
iter:2800 | loss: 213.567
iter:2900 | loss: 212.484
iter:3000 | loss: 174.889
iter:3100 | loss: 237.948
iter:3200 | loss: 132.436
iter:3300 | loss: 130.383
iter:3400 | loss: 134.550
iter:3500 | loss: 166.814
iter:3600 | loss: 144.589
iter:3700 | loss: 132.082
iter:3800 | loss: 150.460
iter:3900 | loss: 204.794
| end of epoch   5 | time: 856.72s | train_loss 161.7289 | valid rse 0.0527 | valid rae 0.0375 | valid corr  0.9330
test rse 0.0796 | test rae 0.0447 | test corr 0.9405
iter:  0 | loss: 118.848
iter:100 | loss: 110.902
iter:200 | loss: 131.931
iter:300 | loss: 145.559
iter:400 | loss: 186.537
iter:500 | loss: 214.373
iter:600 | loss: 154.641
iter:700 | loss: 144.216
iter:800 | loss: 187.621
iter:900 | loss: 259.542
iter:1000 | loss: 165.281
iter:1100 | loss: 242.093
iter:1200 | loss: 121.062
iter:1300 | loss: 202.391
iter:1400 | loss: 151.895
iter:1500 | loss: 128.971
iter:1600 | loss: 193.953
iter:1700 | loss: 102.327
iter:1800 | loss: 130.683
iter:1900 | loss: 189.798
iter:2000 | loss: 211.811
iter:2100 | loss: 137.981
iter:2200 | loss: 112.925
iter:2300 | loss: 127.234
iter:2400 | loss: 120.481
iter:2500 | loss: 167.089
iter:2600 | loss: 175.144
iter:2700 | loss: 137.353
iter:2800 | loss: 175.964
iter:2900 | loss: 127.739
iter:3000 | loss: 147.762
iter:3100 | loss: 150.874
iter:3200 | loss: 115.783
iter:3300 | loss: 156.472
iter:3400 | loss: 203.735
iter:3500 | loss: 287.146
iter:3600 | loss: 120.561
iter:3700 | loss: 199.761
iter:3800 | loss: 162.293
iter:3900 | loss: 110.873
| end of epoch   6 | time: 855.93s | train_loss 160.0600 | valid rse 0.0564 | valid rae 0.0391 | valid corr  0.9322
iter:  0 | loss: 138.943
iter:100 | loss: 111.986
iter:200 | loss: 189.847
iter:300 | loss: 160.773
iter:400 | loss: 132.290
iter:500 | loss: 186.692
iter:600 | loss: 118.091
iter:700 | loss: 199.155
iter:800 | loss: 168.648
iter:900 | loss: 122.644
iter:1000 | loss: 169.941
iter:1100 | loss: 117.051
iter:1200 | loss: 99.736
iter:1300 | loss: 111.343
iter:1400 | loss: 110.108
iter:1500 | loss: 145.608
iter:1600 | loss: 111.829
iter:1700 | loss: 214.657
iter:1800 | loss: 149.099
iter:1900 | loss: 150.722
iter:2000 | loss: 165.319
iter:2100 | loss: 177.430
iter:2200 | loss: 119.967
iter:2300 | loss: 244.816
iter:2400 | loss: 177.736
iter:2500 | loss: 150.290
iter:2600 | loss: 129.480
iter:2700 | loss: 159.186
iter:2800 | loss: 119.119
iter:2900 | loss: 109.976
iter:3000 | loss: 209.431
iter:3100 | loss: 206.321
iter:3200 | loss: 140.310
iter:3300 | loss: 173.735
iter:3400 | loss: 121.617
iter:3500 | loss: 156.150
iter:3600 | loss: 138.407
iter:3700 | loss: 148.778
iter:3800 | loss: 169.287
iter:3900 | loss: 195.934
| end of epoch   7 | time: 856.48s | train_loss 157.9119 | valid rse 0.0523 | valid rae 0.0374 | valid corr  0.9344
iter:  0 | loss: 162.154
iter:100 | loss: 214.310
iter:200 | loss: 147.670
iter:300 | loss: 117.040
iter:400 | loss: 144.642
iter:500 | loss: 147.362
iter:600 | loss: 109.233
iter:700 | loss: 200.785
iter:800 | loss: 130.334
iter:900 | loss: 170.476
iter:1000 | loss: 145.038
iter:1100 | loss: 125.043
iter:1200 | loss: 229.765
iter:1300 | loss: 178.378
iter:1400 | loss: 187.312
iter:1500 | loss: 118.321
iter:1600 | loss: 141.711
iter:1700 | loss: 280.119
iter:1800 | loss: 129.305
iter:1900 | loss: 236.675
iter:2000 | loss: 131.773
iter:2100 | loss: 117.795
iter:2200 | loss: 132.184
iter:2300 | loss: 204.607
iter:2400 | loss: 139.988
iter:2500 | loss: 176.993
iter:2600 | loss: 131.877
iter:2700 | loss: 137.377
iter:2800 | loss: 141.864
iter:2900 | loss: 133.634
iter:3000 | loss: 129.821
iter:3100 | loss: 110.389
iter:3200 | loss: 125.399
iter:3300 | loss: 184.530
iter:3400 | loss: 173.553
iter:3500 | loss: 187.192
iter:3600 | loss: 129.864
iter:3700 | loss: 158.760
iter:3800 | loss: 168.226
iter:3900 | loss: 135.990
| end of epoch   8 | time: 857.47s | train_loss 156.2311 | valid rse 0.0525 | valid rae 0.0373 | valid corr  0.9367
iter:  0 | loss: 159.072
iter:100 | loss: 270.116
iter:200 | loss: 168.872
iter:300 | loss: 156.216
iter:400 | loss: 136.215
iter:500 | loss: 124.679
iter:600 | loss: 215.720
iter:700 | loss: 245.163
iter:800 | loss: 145.913
iter:900 | loss: 137.330
iter:1000 | loss: 99.567
iter:1100 | loss: 114.180
iter:1200 | loss: 121.716
iter:1300 | loss: 162.415
iter:1400 | loss: 211.370
iter:1500 | loss: 146.315
iter:1600 | loss: 109.133
iter:1700 | loss: 276.427
iter:1800 | loss: 210.826
iter:1900 | loss: 91.301
iter:2000 | loss: 157.357
iter:2100 | loss: 123.575
iter:2200 | loss: 138.041
iter:2300 | loss: 203.372
iter:2400 | loss: 131.355
iter:2500 | loss: 120.239
iter:2600 | loss: 227.732
iter:2700 | loss: 120.867
iter:2800 | loss: 134.864
iter:2900 | loss: 94.645
iter:3000 | loss: 128.728
iter:3100 | loss: 211.864
iter:3200 | loss: 237.866
iter:3300 | loss: 138.364
iter:3400 | loss: 192.501
iter:3500 | loss: 191.411
iter:3600 | loss: 138.763
iter:3700 | loss: 172.952
iter:3800 | loss: 115.098
iter:3900 | loss: 161.741
| end of epoch   9 | time: 856.67s | train_loss 154.7635 | valid rse 0.0520 | valid rae 0.0365 | valid corr  0.9373
iter:  0 | loss: 174.471
iter:100 | loss: 179.656
iter:200 | loss: 117.392
iter:300 | loss: 127.392
iter:400 | loss: 122.921
iter:500 | loss: 196.813
iter:600 | loss: 134.619
iter:700 | loss: 107.628
iter:800 | loss: 133.538
iter:900 | loss: 152.686
iter:1000 | loss: 179.990
iter:1100 | loss: 171.128
iter:1200 | loss: 228.394
iter:1300 | loss: 221.050
iter:1400 | loss: 152.754
iter:1500 | loss: 87.783
iter:1600 | loss: 148.658
iter:1700 | loss: 113.965
iter:1800 | loss: 144.126
iter:1900 | loss: 167.384
iter:2000 | loss: 134.028
iter:2100 | loss: 136.248
iter:2200 | loss: 145.873
iter:2300 | loss: 118.617
iter:2400 | loss: 133.360
iter:2500 | loss: 131.752
iter:2600 | loss: 106.926
iter:2700 | loss: 192.251
iter:2800 | loss: 160.372
iter:2900 | loss: 114.810
iter:3000 | loss: 138.001
iter:3100 | loss: 243.123
iter:3200 | loss: 177.764
iter:3300 | loss: 105.638
iter:3400 | loss: 120.840
iter:3500 | loss: 170.866
iter:3600 | loss: 146.782
iter:3700 | loss: 129.897
iter:3800 | loss: 128.430
iter:3900 | loss: 274.370
| end of epoch  10 | time: 855.82s | train_loss 153.9247 | valid rse 0.0510 | valid rae 0.0358 | valid corr  0.9375
test rse 0.0760 | test rae 0.0427 | test corr 0.9445
iter:  0 | loss: 145.276
iter:100 | loss: 96.153
iter:200 | loss: 216.133
iter:300 | loss: 179.425
iter:400 | loss: 209.335
iter:500 | loss: 111.101
iter:600 | loss: 128.761
iter:700 | loss: 121.705
iter:800 | loss: 171.302
iter:900 | loss: 131.398
iter:1000 | loss: 121.442
iter:1100 | loss: 140.156
iter:1200 | loss: 154.069
iter:1300 | loss: 157.085
iter:1400 | loss: 105.360
iter:1500 | loss: 136.163
iter:1600 | loss: 147.692
iter:1700 | loss: 144.676
iter:1800 | loss: 127.210
iter:1900 | loss: 141.410
iter:2000 | loss: 111.501
iter:2100 | loss: 113.321
iter:2200 | loss: 146.810
iter:2300 | loss: 218.845
iter:2400 | loss: 117.033
iter:2500 | loss: 201.130
iter:2600 | loss: 113.005
iter:2700 | loss: 204.499
iter:2800 | loss: 126.683
iter:2900 | loss: 138.936
iter:3000 | loss: 145.643
iter:3100 | loss: 98.688
iter:3200 | loss: 114.801
iter:3300 | loss: 120.697
iter:3400 | loss: 121.791
iter:3500 | loss: 230.612
iter:3600 | loss: 92.090
iter:3700 | loss: 156.909
iter:3800 | loss: 129.991
iter:3900 | loss: 133.524
| end of epoch  11 | time: 854.29s | train_loss 152.5879 | valid rse 0.0509 | valid rae 0.0357 | valid corr  0.9382
iter:  0 | loss: 161.936
iter:100 | loss: 135.176
iter:200 | loss: 132.869
iter:300 | loss: 134.061
iter:400 | loss: 196.368
iter:500 | loss: 166.118
iter:600 | loss: 114.562
iter:700 | loss: 293.266
iter:800 | loss: 162.102
iter:900 | loss: 116.141
iter:1000 | loss: 174.685
iter:1100 | loss: 208.085
iter:1200 | loss: 163.926
iter:1300 | loss: 150.681
iter:1400 | loss: 131.796
iter:1500 | loss: 116.899
iter:1600 | loss: 119.944
iter:1700 | loss: 136.126
iter:1800 | loss: 112.396
iter:1900 | loss: 174.677
iter:2000 | loss: 147.171
iter:2100 | loss: 167.678
iter:2200 | loss: 100.946
iter:2300 | loss: 159.236
iter:2400 | loss: 125.402
iter:2500 | loss: 166.126
iter:2600 | loss: 147.751
iter:2700 | loss: 136.579
iter:2800 | loss: 169.445
iter:2900 | loss: 139.991
iter:3000 | loss: 190.599
iter:3100 | loss: 206.377
iter:3200 | loss: 109.158
iter:3300 | loss: 115.048
iter:3400 | loss: 166.424
iter:3500 | loss: 125.719
iter:3600 | loss: 162.106
iter:3700 | loss: 105.737
iter:3800 | loss: 139.252
iter:3900 | loss: 184.961
| end of epoch  12 | time: 855.20s | train_loss 151.5092 | valid rse 0.0512 | valid rae 0.0363 | valid corr  0.9381
iter:  0 | loss: 142.747
iter:100 | loss: 186.137
iter:200 | loss: 106.060
iter:300 | loss: 144.641
iter:400 | loss: 140.973
iter:500 | loss: 154.951
iter:600 | loss: 115.825
iter:700 | loss: 114.203
iter:800 | loss: 180.319
iter:900 | loss: 186.352
iter:1000 | loss: 132.022
iter:1100 | loss: 127.080
iter:1200 | loss: 159.715
iter:1300 | loss: 153.143
iter:1400 | loss: 123.013
iter:1500 | loss: 108.257
iter:1600 | loss: 136.209
iter:1700 | loss: 94.717
iter:1800 | loss: 155.648
iter:1900 | loss: 148.387
iter:2000 | loss: 152.591
iter:2100 | loss: 196.984
iter:2200 | loss: 159.964
iter:2300 | loss: 117.092
iter:2400 | loss: 115.344
iter:2500 | loss: 117.145
iter:2600 | loss: 171.463
iter:2700 | loss: 161.686
iter:2800 | loss: 150.137
iter:2900 | loss: 111.400
iter:3000 | loss: 128.202
iter:3100 | loss: 138.320
iter:3200 | loss: 91.126
iter:3300 | loss: 127.135
iter:3400 | loss: 190.725
iter:3500 | loss: 113.713
iter:3600 | loss: 109.699
iter:3700 | loss: 130.241
iter:3800 | loss: 142.106
iter:3900 | loss: 148.376
| end of epoch  13 | time: 856.88s | train_loss 151.0949 | valid rse 0.0572 | valid rae 0.0406 | valid corr  0.9387
iter:  0 | loss: 134.212
iter:100 | loss: 182.247
iter:200 | loss: 120.122
iter:300 | loss: 137.235
iter:400 | loss: 221.677
iter:500 | loss: 166.812
iter:600 | loss: 121.145
iter:700 | loss: 117.321
iter:800 | loss: 104.316
iter:900 | loss: 194.267
iter:1000 | loss: 238.856
iter:1100 | loss: 107.541
iter:1200 | loss: 108.379
iter:1300 | loss: 169.642
iter:1400 | loss: 120.583
iter:1500 | loss: 120.662
iter:1600 | loss: 142.515
iter:1700 | loss: 140.750
iter:1800 | loss: 174.271
iter:1900 | loss: 131.420
iter:2000 | loss: 139.212
iter:2100 | loss: 121.943
iter:2200 | loss: 252.463
iter:2300 | loss: 122.920
iter:2400 | loss: 201.974
iter:2500 | loss: 184.442
iter:2600 | loss: 161.429
iter:2700 | loss: 137.839
iter:2800 | loss: 182.649
iter:2900 | loss: 181.778
iter:3000 | loss: 146.976
iter:3100 | loss: 115.131
iter:3200 | loss: 164.758
iter:3300 | loss: 137.460
iter:3400 | loss: 209.167
iter:3500 | loss: 164.915
iter:3600 | loss: 134.462
iter:3700 | loss: 154.262
iter:3800 | loss: 124.551
iter:3900 | loss: 214.374
| end of epoch  14 | time: 856.62s | train_loss 150.4667 | valid rse 0.0531 | valid rae 0.0366 | valid corr  0.9385
iter:  0 | loss: 115.081
iter:100 | loss: 132.015
iter:200 | loss: 190.753
iter:300 | loss: 122.002
iter:400 | loss: 135.904
iter:500 | loss: 229.783
iter:600 | loss: 109.056
iter:700 | loss: 93.844
iter:800 | loss: 144.973
iter:900 | loss: 224.446
iter:1000 | loss: 192.664
iter:1100 | loss: 122.120
iter:1200 | loss: 113.013
iter:1300 | loss: 131.609
iter:1400 | loss: 156.254
iter:1500 | loss: 205.501
iter:1600 | loss: 141.023
iter:1700 | loss: 156.208
iter:1800 | loss: 220.429
iter:1900 | loss: 160.391
iter:2000 | loss: 180.498
iter:2100 | loss: 107.613
iter:2200 | loss: 219.621
iter:2300 | loss: 177.129
iter:2400 | loss: 224.796
iter:2500 | loss: 159.406
iter:2600 | loss: 147.298
iter:2700 | loss: 134.247
iter:2800 | loss: 211.341
iter:2900 | loss: 153.869
iter:3000 | loss: 108.092
iter:3100 | loss: 117.454
iter:3200 | loss: 119.352
iter:3300 | loss: 240.153
iter:3400 | loss: 134.831
iter:3500 | loss: 167.535
iter:3600 | loss: 121.796
iter:3700 | loss: 141.530
iter:3800 | loss: 193.289
iter:3900 | loss: 192.588
| end of epoch  15 | time: 855.96s | train_loss 149.6250 | valid rse 0.0510 | valid rae 0.0356 | valid corr  0.9399
test rse 0.0753 | test rae 0.0426 | test corr 0.9460
iter:  0 | loss: 385.274
iter:100 | loss: 169.818
iter:200 | loss: 114.268
iter:300 | loss: 124.840
iter:400 | loss: 122.504
iter:500 | loss: 169.953
iter:600 | loss: 121.608
iter:700 | loss: 129.248
iter:800 | loss: 167.797
iter:900 | loss: 175.734
iter:1000 | loss: 229.535
iter:1100 | loss: 143.827
iter:1200 | loss: 138.853
iter:1300 | loss: 182.916
iter:1400 | loss: 189.373
iter:1500 | loss: 143.714
iter:1600 | loss: 176.075
iter:1700 | loss: 176.371
iter:1800 | loss: 153.821
iter:1900 | loss: 144.315
iter:2000 | loss: 145.416
iter:2100 | loss: 196.821
iter:2200 | loss: 188.554
iter:2300 | loss: 121.894
iter:2400 | loss: 142.038
iter:2500 | loss: 136.832
iter:2600 | loss: 136.417
iter:2700 | loss: 182.179
iter:2800 | loss: 111.728
iter:2900 | loss: 178.458
iter:3000 | loss: 168.195
iter:3100 | loss: 160.260
iter:3200 | loss: 154.818
iter:3300 | loss: 200.314
iter:3400 | loss: 162.105
iter:3500 | loss: 119.781
iter:3600 | loss: 154.136
iter:3700 | loss: 130.161
iter:3800 | loss: 147.086
iter:3900 | loss: 187.834
| end of epoch  16 | time: 855.90s | train_loss 148.7706 | valid rse 0.0517 | valid rae 0.0371 | valid corr  0.9382
iter:  0 | loss: 103.059
iter:100 | loss: 145.843
iter:200 | loss: 90.295
iter:300 | loss: 144.987
iter:400 | loss: 146.985
iter:500 | loss: 126.137
iter:600 | loss: 152.977
iter:700 | loss: 125.037
iter:800 | loss: 122.537
iter:900 | loss: 110.358
iter:1000 | loss: 121.421
iter:1100 | loss: 118.266
iter:1200 | loss: 184.667
iter:1300 | loss: 148.298
iter:1400 | loss: 156.052
iter:1500 | loss: 138.101
iter:1600 | loss: 142.206
iter:1700 | loss: 158.335
iter:1800 | loss: 136.054
iter:1900 | loss: 226.332
iter:2000 | loss: 221.537
iter:2100 | loss: 157.521
iter:2200 | loss: 129.225
iter:2300 | loss: 110.603
iter:2400 | loss: 121.379
iter:2500 | loss: 124.630
iter:2600 | loss: 133.776
iter:2700 | loss: 239.173
iter:2800 | loss: 158.692
iter:2900 | loss: 150.172
iter:3000 | loss: 194.577
iter:3100 | loss: 140.383
iter:3200 | loss: 148.628
iter:3300 | loss: 107.627
iter:3400 | loss: 137.972
iter:3500 | loss: 104.474
iter:3600 | loss: 153.926
iter:3700 | loss: 166.212
iter:3800 | loss: 135.374
iter:3900 | loss: 105.521
| end of epoch  17 | time: 857.71s | train_loss 148.4165 | valid rse 0.0521 | valid rae 0.0359 | valid corr  0.9397
iter:  0 | loss: 216.152
iter:100 | loss: 171.318
iter:200 | loss: 117.962
iter:300 | loss: 135.454
iter:400 | loss: 116.710
iter:500 | loss: 128.741
iter:600 | loss: 111.041
iter:700 | loss: 141.819
iter:800 | loss: 145.462
iter:900 | loss: 177.105
iter:1000 | loss: 173.836
iter:1100 | loss: 122.992
iter:1200 | loss: 160.082
iter:1300 | loss: 147.312
iter:1400 | loss: 129.871
iter:1500 | loss: 139.728
iter:1600 | loss: 156.975
iter:1700 | loss: 188.664
iter:1800 | loss: 216.936
iter:1900 | loss: 115.147
iter:2000 | loss: 96.968
iter:2100 | loss: 139.727
iter:2200 | loss: 180.147
iter:2300 | loss: 173.077
iter:2400 | loss: 219.080
iter:2500 | loss: 116.296
iter:2600 | loss: 113.969
iter:2700 | loss: 121.481
iter:2800 | loss: 134.218
iter:2900 | loss: 165.889
iter:3000 | loss: 208.258
iter:3100 | loss: 131.258
iter:3200 | loss: 188.344
iter:3300 | loss: 117.529
iter:3400 | loss: 161.774
iter:3500 | loss: 191.471
iter:3600 | loss: 104.738
iter:3700 | loss: 163.153
iter:3800 | loss: 121.294
iter:3900 | loss: 114.266
| end of epoch  18 | time: 856.00s | train_loss 148.0834 | valid rse 0.0507 | valid rae 0.0354 | valid corr  0.9402
iter:  0 | loss: 204.621
iter:100 | loss: 137.412
iter:200 | loss: 148.137
iter:300 | loss: 155.795
iter:400 | loss: 153.001
iter:500 | loss: 97.747
iter:600 | loss: 186.291
iter:700 | loss: 141.817
iter:800 | loss: 211.255
iter:900 | loss: 185.404
iter:1000 | loss: 122.711
iter:1100 | loss: 125.152
iter:1200 | loss: 193.932
iter:1300 | loss: 237.643
iter:1400 | loss: 173.306
iter:1500 | loss: 272.159
iter:1600 | loss: 140.516
iter:1700 | loss: 102.362
iter:1800 | loss: 158.838
iter:1900 | loss: 149.786
iter:2000 | loss: 99.545
iter:2100 | loss: 115.286
iter:2200 | loss: 134.503
iter:2300 | loss: 169.795
iter:2400 | loss: 117.968
iter:2500 | loss: 225.642
iter:2600 | loss: 139.580
iter:2700 | loss: 178.440
iter:2800 | loss: 142.892
iter:2900 | loss: 118.975
iter:3000 | loss: 125.911
iter:3100 | loss: 154.166
iter:3200 | loss: 175.835
iter:3300 | loss: 113.631
iter:3400 | loss: 118.657
iter:3500 | loss: 95.865
iter:3600 | loss: 106.295
iter:3700 | loss: 149.950
iter:3800 | loss: 129.621
iter:3900 | loss: 115.356
| end of epoch  19 | time: 856.51s | train_loss 147.8302 | valid rse 0.0503 | valid rae 0.0351 | valid corr  0.9399
iter:  0 | loss: 150.307
iter:100 | loss: 133.401
iter:200 | loss: 112.339
iter:300 | loss: 154.988
iter:400 | loss: 135.246
iter:500 | loss: 165.393
iter:600 | loss: 128.775
iter:700 | loss: 152.398
iter:800 | loss: 128.942
iter:900 | loss: 358.948
iter:1000 | loss: 133.013
iter:1100 | loss: 197.076
iter:1200 | loss: 119.375
iter:1300 | loss: 139.124
iter:1400 | loss: 113.920
iter:1500 | loss: 138.008
iter:1600 | loss: 140.417
iter:1700 | loss: 99.063
iter:1800 | loss: 118.643
iter:1900 | loss: 275.122
iter:2000 | loss: 98.480
iter:2100 | loss: 130.472
iter:2200 | loss: 115.944
iter:2300 | loss: 172.024
iter:2400 | loss: 146.532
iter:2500 | loss: 84.030
iter:2600 | loss: 159.783
iter:2700 | loss: 160.486
iter:2800 | loss: 139.823
iter:2900 | loss: 204.709
iter:3000 | loss: 173.215
iter:3100 | loss: 135.870
iter:3200 | loss: 95.989
iter:3300 | loss: 174.446
iter:3400 | loss: 138.646
iter:3500 | loss: 164.385
iter:3600 | loss: 103.602
iter:3700 | loss: 124.759
iter:3800 | loss: 138.792
iter:3900 | loss: 137.404
| end of epoch  20 | time: 856.80s | train_loss 147.1274 | valid rse 0.0503 | valid rae 0.0357 | valid corr  0.9395
test rse 0.0764 | test rae 0.0429 | test corr 0.9461
iter:  0 | loss: 145.645
iter:100 | loss: 191.549
iter:200 | loss: 114.869
iter:300 | loss: 242.425
iter:400 | loss: 133.671
iter:500 | loss: 103.034
iter:600 | loss: 132.741
iter:700 | loss: 135.440
iter:800 | loss: 180.192
iter:900 | loss: 130.332
iter:1000 | loss: 150.134
iter:1100 | loss: 126.930
iter:1200 | loss: 215.962
iter:1300 | loss: 120.306
iter:1400 | loss: 164.822
iter:1500 | loss: 108.891
iter:1600 | loss: 169.354
iter:1700 | loss: 200.589
iter:1800 | loss: 176.574
iter:1900 | loss: 115.496
iter:2000 | loss: 116.219
iter:2100 | loss: 106.912
iter:2200 | loss: 109.352
iter:2300 | loss: 175.652
iter:2400 | loss: 108.092
iter:2500 | loss: 89.830
iter:2600 | loss: 228.192
iter:2700 | loss: 215.669
iter:2800 | loss: 191.053
iter:2900 | loss: 205.315
iter:3000 | loss: 182.960
iter:3100 | loss: 150.437
iter:3200 | loss: 124.112
iter:3300 | loss: 121.882
iter:3400 | loss: 156.173
iter:3500 | loss: 129.880
iter:3600 | loss: 123.843
iter:3700 | loss: 137.800
iter:3800 | loss: 131.283
iter:3900 | loss: 97.255
| end of epoch  21 | time: 855.16s | train_loss 146.2118 | valid rse 0.0524 | valid rae 0.0367 | valid corr  0.9389
iter:  0 | loss: 122.108
iter:100 | loss: 199.609
iter:200 | loss: 143.456
iter:300 | loss: 136.625
iter:400 | loss: 140.046
iter:500 | loss: 150.776
iter:600 | loss: 130.038
iter:700 | loss: 277.264
iter:800 | loss: 91.569
iter:900 | loss: 118.357
iter:1000 | loss: 176.663
iter:1100 | loss: 119.360
iter:1200 | loss: 97.076
iter:1300 | loss: 119.045
iter:1400 | loss: 131.754
iter:1500 | loss: 139.823
iter:1600 | loss: 121.491
iter:1700 | loss: 143.725
iter:1800 | loss: 147.331
iter:1900 | loss: 106.789
iter:2000 | loss: 102.090
iter:2100 | loss: 156.590
iter:2200 | loss: 186.852
iter:2300 | loss: 148.405
iter:2400 | loss: 192.262
iter:2500 | loss: 231.647
iter:2600 | loss: 126.008
iter:2700 | loss: 97.356
iter:2800 | loss: 161.313
iter:2900 | loss: 126.517
iter:3000 | loss: 118.365
iter:3100 | loss: 142.614
iter:3200 | loss: 199.610
iter:3300 | loss: 135.102
iter:3400 | loss: 124.724
iter:3500 | loss: 165.660
iter:3600 | loss: 119.242
iter:3700 | loss: 121.796
iter:3800 | loss: 125.242
iter:3900 | loss: 105.475
| end of epoch  22 | time: 854.93s | train_loss 146.4845 | valid rse 0.0503 | valid rae 0.0348 | valid corr  0.9401
iter:  0 | loss: 119.164
iter:100 | loss: 216.169
iter:200 | loss: 179.056
iter:300 | loss: 88.552
iter:400 | loss: 168.790
iter:500 | loss: 122.670
iter:600 | loss: 157.234
iter:700 | loss: 133.725
iter:800 | loss: 165.538
iter:900 | loss: 103.853
iter:1000 | loss: 110.441
iter:1100 | loss: 131.549
iter:1200 | loss: 116.622
iter:1300 | loss: 122.943
iter:1400 | loss: 140.701
iter:1500 | loss: 190.452
iter:1600 | loss: 127.817
iter:1700 | loss: 185.937
iter:1800 | loss: 145.639
iter:1900 | loss: 146.301
iter:2000 | loss: 161.983
iter:2100 | loss: 176.713
iter:2200 | loss: 146.069
iter:2300 | loss: 131.455
iter:2400 | loss: 138.065
iter:2500 | loss: 149.797
iter:2600 | loss: 105.058
iter:2700 | loss: 170.620
iter:2800 | loss: 164.127
iter:2900 | loss: 171.485
iter:3000 | loss: 132.541
iter:3100 | loss: 133.892
iter:3200 | loss: 163.032
iter:3300 | loss: 81.714
iter:3400 | loss: 172.676
iter:3500 | loss: 106.142
iter:3600 | loss: 95.264
iter:3700 | loss: 108.803
iter:3800 | loss: 125.551
iter:3900 | loss: 125.353
| end of epoch  23 | time: 857.70s | train_loss 145.4622 | valid rse 0.0521 | valid rae 0.0392 | valid corr  0.9402
iter:  0 | loss: 118.725
iter:100 | loss: 133.835
iter:200 | loss: 184.550
iter:300 | loss: 146.293
iter:400 | loss: 131.506
iter:500 | loss: 130.879
iter:600 | loss: 143.273
iter:700 | loss: 93.305
iter:800 | loss: 200.698
iter:900 | loss: 115.786
iter:1000 | loss: 132.223
iter:1100 | loss: 140.703
iter:1200 | loss: 196.382
iter:1300 | loss: 182.147
iter:1400 | loss: 128.128
iter:1500 | loss: 208.520
iter:1600 | loss: 220.986
iter:1700 | loss: 85.365
iter:1800 | loss: 104.696
iter:1900 | loss: 138.306
iter:2000 | loss: 152.500
iter:2100 | loss: 171.605
iter:2200 | loss: 141.164
iter:2300 | loss: 96.614
iter:2400 | loss: 92.717
iter:2500 | loss: 203.367
iter:2600 | loss: 120.050
iter:2700 | loss: 112.681
iter:2800 | loss: 123.753
iter:2900 | loss: 129.785
iter:3000 | loss: 194.409
iter:3100 | loss: 147.303
iter:3200 | loss: 116.986
iter:3300 | loss: 136.353
iter:3400 | loss: 169.285
iter:3500 | loss: 129.876
iter:3600 | loss: 109.493
iter:3700 | loss: 114.009
iter:3800 | loss: 151.795
iter:3900 | loss: 92.247
| end of epoch  24 | time: 858.74s | train_loss 145.4611 | valid rse 0.0512 | valid rae 0.0362 | valid corr  0.9397
iter:  0 | loss: 132.897
iter:100 | loss: 120.611
iter:200 | loss: 143.958
iter:300 | loss: 114.557
iter:400 | loss: 100.347
iter:500 | loss: 194.968
iter:600 | loss: 180.085
iter:700 | loss: 184.262
iter:800 | loss: 133.492
iter:900 | loss: 120.304
iter:1000 | loss: 126.882
iter:1100 | loss: 104.533
iter:1200 | loss: 114.434
iter:1300 | loss: 153.360
iter:1400 | loss: 128.332
iter:1500 | loss: 155.304
iter:1600 | loss: 124.498
iter:1700 | loss: 161.620
iter:1800 | loss: 90.297
iter:1900 | loss: 290.337
iter:2000 | loss: 113.782
iter:2100 | loss: 162.354
iter:2200 | loss: 177.645
iter:2300 | loss: 129.650
iter:2400 | loss: 139.053
iter:2500 | loss: 142.151
iter:2600 | loss: 153.842
iter:2700 | loss: 91.991
iter:2800 | loss: 106.977
iter:2900 | loss: 148.002
iter:3000 | loss: 171.396
iter:3100 | loss: 138.804
iter:3200 | loss: 249.111
iter:3300 | loss: 178.051
iter:3400 | loss: 136.167
iter:3500 | loss: 139.953
iter:3600 | loss: 103.312
iter:3700 | loss: 153.105
iter:3800 | loss: 110.081
iter:3900 | loss: 155.594
| end of epoch  25 | time: 858.03s | train_loss 145.2807 | valid rse 0.0507 | valid rae 0.0348 | valid corr  0.9408
test rse 0.0739 | test rae 0.0413 | test corr 0.9468
iter:  0 | loss: 196.930
iter:100 | loss: 229.701
iter:200 | loss: 169.879
iter:300 | loss: 106.490
iter:400 | loss: 121.521
iter:500 | loss: 172.441
iter:600 | loss: 121.128
iter:700 | loss: 110.437
iter:800 | loss: 98.253
iter:900 | loss: 122.748
iter:1000 | loss: 135.287
iter:1100 | loss: 262.032
iter:1200 | loss: 118.659
iter:1300 | loss: 167.478
iter:1400 | loss: 114.721
iter:1500 | loss: 144.435
iter:1600 | loss: 215.142
iter:1700 | loss: 133.775
iter:1800 | loss: 184.578
iter:1900 | loss: 145.551
iter:2000 | loss: 145.464
iter:2100 | loss: 167.893
iter:2200 | loss: 147.153
iter:2300 | loss: 98.584
iter:2400 | loss: 104.570
iter:2500 | loss: 158.827
iter:2600 | loss: 103.914
iter:2700 | loss: 184.724
iter:2800 | loss: 317.079
iter:2900 | loss: 147.090
iter:3000 | loss: 186.087
iter:3100 | loss: 158.569
iter:3200 | loss: 115.908
iter:3300 | loss: 111.561
iter:3400 | loss: 145.750
iter:3500 | loss: 161.877
iter:3600 | loss: 154.268
iter:3700 | loss: 171.296
iter:3800 | loss: 104.801
iter:3900 | loss: 137.165
| end of epoch  26 | time: 857.94s | train_loss 144.6347 | valid rse 0.0514 | valid rae 0.0362 | valid corr  0.9391
iter:  0 | loss: 107.301
iter:100 | loss: 174.670
iter:200 | loss: 158.238
iter:300 | loss: 118.976
iter:400 | loss: 101.163
iter:500 | loss: 205.568
iter:600 | loss: 161.178
iter:700 | loss: 147.487
iter:800 | loss: 140.565
iter:900 | loss: 152.926
iter:1000 | loss: 184.168
iter:1100 | loss: 109.877
iter:1200 | loss: 196.980
iter:1300 | loss: 145.876
iter:1400 | loss: 126.277
iter:1500 | loss: 179.308
iter:1600 | loss: 166.726
iter:1700 | loss: 133.914
iter:1800 | loss: 127.615
iter:1900 | loss: 149.554
iter:2000 | loss: 112.530
iter:2100 | loss: 140.690
iter:2200 | loss: 132.282
iter:2300 | loss: 139.984
iter:2400 | loss: 146.202
iter:2500 | loss: 148.191
iter:2600 | loss: 204.377
iter:2700 | loss: 177.660
iter:2800 | loss: 462.854
iter:2900 | loss: 80.183
iter:3000 | loss: 100.153
iter:3100 | loss: 125.655
iter:3200 | loss: 211.781
iter:3300 | loss: 185.924
iter:3400 | loss: 165.214
iter:3500 | loss: 102.337
iter:3600 | loss: 113.219
iter:3700 | loss: 159.550
iter:3800 | loss: 273.226
iter:3900 | loss: 112.560
| end of epoch  27 | time: 858.53s | train_loss 144.7156 | valid rse 0.0504 | valid rae 0.0357 | valid corr  0.9408
iter:  0 | loss: 161.376
iter:100 | loss: 132.600
iter:200 | loss: 131.511
iter:300 | loss: 111.046
iter:400 | loss: 126.310
iter:500 | loss: 112.137
iter:600 | loss: 123.389
iter:700 | loss: 166.931
iter:800 | loss: 143.517
iter:900 | loss: 150.061
iter:1000 | loss: 161.956
iter:1100 | loss: 115.256
iter:1200 | loss: 108.174
iter:1300 | loss: 101.124
iter:1400 | loss: 118.590
iter:1500 | loss: 130.402
iter:1600 | loss: 123.942
iter:1700 | loss: 167.104
iter:1800 | loss: 114.997
iter:1900 | loss: 127.184
iter:2000 | loss: 158.023
iter:2100 | loss: 90.807
iter:2200 | loss: 97.937
iter:2300 | loss: 143.323
iter:2400 | loss: 118.396
iter:2500 | loss: 166.965
iter:2600 | loss: 138.695
iter:2700 | loss: 126.313
iter:2800 | loss: 107.496
iter:2900 | loss: 138.821
iter:3000 | loss: 107.229
iter:3100 | loss: 194.086
iter:3200 | loss: 151.831
iter:3300 | loss: 116.687
iter:3400 | loss: 140.797
iter:3500 | loss: 100.777
iter:3600 | loss: 96.943
iter:3700 | loss: 199.263
iter:3800 | loss: 131.204
iter:3900 | loss: 103.990
| end of epoch  28 | time: 857.91s | train_loss 144.1015 | valid rse 0.0504 | valid rae 0.0347 | valid corr  0.9409
iter:  0 | loss: 206.462
iter:100 | loss: 239.881
iter:200 | loss: 116.924
iter:300 | loss: 90.275
iter:400 | loss: 150.149
iter:500 | loss: 154.841
iter:600 | loss: 292.477
iter:700 | loss: 93.840
iter:800 | loss: 235.151
iter:900 | loss: 99.768
iter:1000 | loss: 111.105
iter:1100 | loss: 120.610
iter:1200 | loss: 185.097
iter:1300 | loss: 132.652
iter:1400 | loss: 139.000
iter:1500 | loss: 168.831
iter:1600 | loss: 176.683
iter:1700 | loss: 129.445
iter:1800 | loss: 153.169
iter:1900 | loss: 183.909
iter:2000 | loss: 131.102
iter:2100 | loss: 110.026
iter:2200 | loss: 129.874
iter:2300 | loss: 106.881
iter:2400 | loss: 114.276
iter:2500 | loss: 147.339
iter:2600 | loss: 119.100
iter:2700 | loss: 112.499
iter:2800 | loss: 144.623
iter:2900 | loss: 148.001
iter:3000 | loss: 118.418
iter:3100 | loss: 129.771
iter:3200 | loss: 100.616
iter:3300 | loss: 145.225
iter:3400 | loss: 185.473
iter:3500 | loss: 137.129
iter:3600 | loss: 141.485
iter:3700 | loss: 198.286
iter:3800 | loss: 147.677
iter:3900 | loss: 178.626
| end of epoch  29 | time: 858.88s | train_loss 143.8908 | valid rse 0.0493 | valid rae 0.0346 | valid corr  0.9403
iter:  0 | loss: 146.947
iter:100 | loss: 107.566
iter:200 | loss: 260.758
iter:300 | loss: 128.562
iter:400 | loss: 163.315
iter:500 | loss: 153.468
iter:600 | loss: 147.713
iter:700 | loss: 145.584
iter:800 | loss: 139.292
iter:900 | loss: 146.510
iter:1000 | loss: 143.986
iter:1100 | loss: 89.237
iter:1200 | loss: 141.148
iter:1300 | loss: 137.967
iter:1400 | loss: 207.830
iter:1500 | loss: 172.656
iter:1600 | loss: 108.406
iter:1700 | loss: 146.528
iter:1800 | loss: 203.604
iter:1900 | loss: 95.862
iter:2000 | loss: 186.749
iter:2100 | loss: 130.859
iter:2200 | loss: 125.173
iter:2300 | loss: 172.507
iter:2400 | loss: 179.474
iter:2500 | loss: 139.088
iter:2600 | loss: 125.571
iter:2700 | loss: 269.043
iter:2800 | loss: 177.247
iter:2900 | loss: 155.218
iter:3000 | loss: 138.094
iter:3100 | loss: 166.166
iter:3200 | loss: 103.082
iter:3300 | loss: 165.019
iter:3400 | loss: 153.092
iter:3500 | loss: 179.091
iter:3600 | loss: 173.110
iter:3700 | loss: 103.391
iter:3800 | loss: 160.934
iter:3900 | loss: 118.989
| end of epoch  30 | time: 858.36s | train_loss 143.3637 | valid rse 0.0497 | valid rae 0.0344 | valid corr  0.9408
test rse 0.0749 | test rae 0.0411 | test corr 0.9474
final test rse 0.0747 | test rae 0.0416 | test corr 0.9468
Namespace(L1Loss=True, batch_size=4, buildA_true=True, clip=5, conv_channels=16, data='./data/electricity.txt', device='cuda:0', dilation_exponential=2, dropout=0.3, end_channels=64, epochs=30, gcn_depth=2, gcn_true=True, horizon=3, in_dim=1, layers=5, log_interval=2000, lr=0.0001, node_dim=40, normalize=2, num_nodes=321, num_split=1, optim='adam', propalpha=0.05, residual_channels=16, save='./models/model-electricity-3.pt', seq_in_len=168, seq_out_len=1, skip_channels=32, step_size=100, subgraph_size=20, tanhalpha=3, weight_decay=1e-05)
The recpetive field size is 187
Number of model parameters is 362385
begin training
iter:  0 | loss: 3009.069
iter:100 | loss: 395.995
iter:200 | loss: 499.616
iter:300 | loss: 258.746
iter:400 | loss: 285.990
iter:500 | loss: 270.915
iter:600 | loss: 166.379
iter:700 | loss: 259.930
iter:800 | loss: 395.061
iter:900 | loss: 211.304
iter:1000 | loss: 214.847
iter:1100 | loss: 190.492
iter:1200 | loss: 189.115
iter:1300 | loss: 214.707
iter:1400 | loss: 174.886
iter:1500 | loss: 272.445
iter:1600 | loss: 175.171
iter:1700 | loss: 285.743
iter:1800 | loss: 209.306
iter:1900 | loss: 177.439
iter:2000 | loss: 230.976
iter:2100 | loss: 212.536
iter:2200 | loss: 269.455
iter:2300 | loss: 208.188
iter:2400 | loss: 202.274
iter:2500 | loss: 210.474
iter:2600 | loss: 169.953
iter:2700 | loss: 202.316
iter:2800 | loss: 174.366
iter:2900 | loss: 180.163
iter:3000 | loss: 256.948
iter:3100 | loss: 137.436
iter:3200 | loss: 198.116
iter:3300 | loss: 200.387
iter:3400 | loss: 123.519
iter:3500 | loss: 151.040
iter:3600 | loss: 138.540
iter:3700 | loss: 174.135
iter:3800 | loss: 162.396
iter:3900 | loss: 165.842
| end of epoch   1 | time: 856.97s | train_loss 241.2323 | valid rse 0.0606 | valid rae 0.0447 | valid corr  0.9185
iter:  0 | loss: 231.293
iter:100 | loss: 173.191
iter:200 | loss: 206.071
iter:300 | loss: 93.762
iter:400 | loss: 163.475
iter:500 | loss: 164.965
iter:600 | loss: 241.499
iter:700 | loss: 147.613
iter:800 | loss: 183.957
iter:900 | loss: 131.799
iter:1000 | loss: 187.131
iter:1100 | loss: 162.216
iter:1200 | loss: 158.314
iter:1300 | loss: 175.119
iter:1400 | loss: 220.675
iter:1500 | loss: 132.228
iter:1600 | loss: 151.652
iter:1700 | loss: 278.070
iter:1800 | loss: 168.020
iter:1900 | loss: 159.222
iter:2000 | loss: 189.448
iter:2100 | loss: 178.768
iter:2200 | loss: 194.161
iter:2300 | loss: 138.661
iter:2400 | loss: 126.892
iter:2500 | loss: 168.399
iter:2600 | loss: 139.309
iter:2700 | loss: 254.222
iter:2800 | loss: 146.863
iter:2900 | loss: 212.035
iter:3000 | loss: 170.724
iter:3100 | loss: 183.186
iter:3200 | loss: 163.191
iter:3300 | loss: 172.881
iter:3400 | loss: 142.432
iter:3500 | loss: 134.525
iter:3600 | loss: 180.551
iter:3700 | loss: 301.368
iter:3800 | loss: 147.081
iter:3900 | loss: 166.841
| end of epoch   2 | time: 857.16s | train_loss 182.3065 | valid rse 0.0606 | valid rae 0.0417 | valid corr  0.9251
iter:  0 | loss: 172.567
iter:100 | loss: 171.357
iter:200 | loss: 156.679
iter:300 | loss: 149.943
iter:400 | loss: 135.808
iter:500 | loss: 177.703
iter:600 | loss: 155.787
iter:700 | loss: 220.771
iter:800 | loss: 161.023
iter:900 | loss: 218.897
iter:1000 | loss: 209.790
iter:1100 | loss: 214.407
iter:1200 | loss: 152.309
iter:1300 | loss: 173.107
iter:1400 | loss: 135.523
iter:1500 | loss: 158.328
iter:1600 | loss: 201.355
iter:1700 | loss: 128.953
iter:1800 | loss: 132.580
iter:1900 | loss: 126.604
iter:2000 | loss: 220.603
iter:2100 | loss: 170.410
iter:2200 | loss: 191.772
iter:2300 | loss: 152.451
iter:2400 | loss: 178.931
iter:2500 | loss: 207.657
iter:2600 | loss: 134.268
iter:2700 | loss: 118.742
iter:2800 | loss: 201.515
iter:2900 | loss: 131.956
iter:3000 | loss: 231.476
iter:3100 | loss: 151.414
iter:3200 | loss: 246.138
iter:3300 | loss: 210.713
iter:3400 | loss: 138.841
iter:3500 | loss: 134.129
iter:3600 | loss: 134.468
iter:3700 | loss: 117.100
iter:3800 | loss: 221.879
iter:3900 | loss: 211.750
| end of epoch   3 | time: 858.88s | train_loss 171.7597 | valid rse 0.0547 | valid rae 0.0387 | valid corr  0.9296
iter:  0 | loss: 185.766
iter:100 | loss: 119.354
iter:200 | loss: 164.900
iter:300 | loss: 129.684
iter:400 | loss: 189.402
iter:500 | loss: 255.022
iter:600 | loss: 165.801
iter:700 | loss: 166.405
iter:800 | loss: 155.227
iter:900 | loss: 142.013
iter:1000 | loss: 188.166
iter:1100 | loss: 245.310
iter:1200 | loss: 175.491
iter:1300 | loss: 209.099
iter:1400 | loss: 264.670
iter:1500 | loss: 152.118
iter:1600 | loss: 172.650
iter:1700 | loss: 153.887
iter:1800 | loss: 165.970
iter:1900 | loss: 124.178
iter:2000 | loss: 134.641
iter:2100 | loss: 154.639
iter:2200 | loss: 179.503
iter:2300 | loss: 211.308
iter:2400 | loss: 146.271
iter:2500 | loss: 115.970
iter:2600 | loss: 176.434
iter:2700 | loss: 135.849
iter:2800 | loss: 116.683
iter:2900 | loss: 146.868
iter:3000 | loss: 139.019
iter:3100 | loss: 150.025
iter:3200 | loss: 130.207
iter:3300 | loss: 135.098
iter:3400 | loss: 136.976
iter:3500 | loss: 111.072
iter:3600 | loss: 141.219
iter:3700 | loss: 112.746
iter:3800 | loss: 144.640
iter:3900 | loss: 150.153
| end of epoch   4 | time: 858.93s | train_loss 166.0643 | valid rse 0.0538 | valid rae 0.0386 | valid corr  0.9293
iter:  0 | loss: 242.129
iter:100 | loss: 273.216
iter:200 | loss: 169.206
iter:300 | loss: 137.564
iter:400 | loss: 160.188
iter:500 | loss: 181.755
iter:600 | loss: 164.760
iter:700 | loss: 228.858
iter:800 | loss: 145.724
iter:900 | loss: 163.776
iter:1000 | loss: 98.194
iter:1100 | loss: 202.905
iter:1200 | loss: 97.027
iter:1300 | loss: 180.396
iter:1400 | loss: 188.088
iter:1500 | loss: 148.134
iter:1600 | loss: 172.680
iter:1700 | loss: 147.900
iter:1800 | loss: 155.884
iter:1900 | loss: 120.376
iter:2000 | loss: 219.688
iter:2100 | loss: 149.137
iter:2200 | loss: 193.029
iter:2300 | loss: 135.728
iter:2400 | loss: 212.947
iter:2500 | loss: 119.399
iter:2600 | loss: 164.860
iter:2700 | loss: 141.435
iter:2800 | loss: 123.927
iter:2900 | loss: 156.489
iter:3000 | loss: 149.668
iter:3100 | loss: 125.788
iter:3200 | loss: 146.258
iter:3300 | loss: 115.666
iter:3400 | loss: 117.323
iter:3500 | loss: 258.556
iter:3600 | loss: 120.617
iter:3700 | loss: 219.831
iter:3800 | loss: 150.079
iter:3900 | loss: 151.467
| end of epoch   5 | time: 858.19s | train_loss 162.6623 | valid rse 0.0540 | valid rae 0.0382 | valid corr  0.9342
test rse 0.0820 | test rae 0.0461 | test corr 0.9417
iter:  0 | loss: 154.778
iter:100 | loss: 164.526
iter:200 | loss: 123.709
iter:300 | loss: 266.078
iter:400 | loss: 170.346
iter:500 | loss: 147.612
iter:600 | loss: 165.027
iter:700 | loss: 183.092
iter:800 | loss: 132.389
iter:900 | loss: 174.782
iter:1000 | loss: 167.552
iter:1100 | loss: 151.744
iter:1200 | loss: 123.638
iter:1300 | loss: 115.679
iter:1400 | loss: 117.160
iter:1500 | loss: 211.719
iter:1600 | loss: 183.883
iter:1700 | loss: 165.627
iter:1800 | loss: 94.888
iter:1900 | loss: 164.107
iter:2000 | loss: 131.705
iter:2100 | loss: 167.839
iter:2200 | loss: 177.205
iter:2300 | loss: 183.202
iter:2400 | loss: 127.048
iter:2500 | loss: 148.355
iter:2600 | loss: 149.698
iter:2700 | loss: 208.625
iter:2800 | loss: 158.848
iter:2900 | loss: 203.114
iter:3000 | loss: 227.891
iter:3100 | loss: 150.327
iter:3200 | loss: 133.208
iter:3300 | loss: 162.539
iter:3400 | loss: 183.078
iter:3500 | loss: 188.731
iter:3600 | loss: 109.245
iter:3700 | loss: 151.504
iter:3800 | loss: 127.808
iter:3900 | loss: 129.582
| end of epoch   6 | time: 858.16s | train_loss 159.8698 | valid rse 0.0529 | valid rae 0.0374 | valid corr  0.9326
iter:  0 | loss: 152.848
iter:100 | loss: 141.731
iter:200 | loss: 148.044
iter:300 | loss: 147.710
iter:400 | loss: 126.127
iter:500 | loss: 134.272
iter:600 | loss: 214.524
iter:700 | loss: 182.157
iter:800 | loss: 143.081
iter:900 | loss: 118.359
iter:1000 | loss: 113.352
iter:1100 | loss: 133.331
iter:1200 | loss: 127.183
iter:1300 | loss: 187.507
iter:1400 | loss: 164.367
iter:1500 | loss: 170.495
iter:1600 | loss: 124.540
iter:1700 | loss: 185.352
iter:1800 | loss: 137.011
iter:1900 | loss: 147.973
iter:2000 | loss: 275.728
iter:2100 | loss: 145.334
iter:2200 | loss: 128.056
iter:2300 | loss: 117.768
iter:2400 | loss: 222.329
iter:2500 | loss: 181.910
iter:2600 | loss: 136.682
iter:2700 | loss: 220.037
iter:2800 | loss: 141.409
iter:2900 | loss: 181.670
iter:3000 | loss: 173.104
iter:3100 | loss: 168.385
iter:3200 | loss: 115.729
iter:3300 | loss: 173.849
iter:3400 | loss: 177.622
iter:3500 | loss: 182.319
iter:3600 | loss: 155.182
iter:3700 | loss: 161.640
iter:3800 | loss: 200.106
iter:3900 | loss: 179.910
| end of epoch   7 | time: 859.62s | train_loss 158.0562 | valid rse 0.0548 | valid rae 0.0400 | valid corr  0.9341
iter:  0 | loss: 147.470
iter:100 | loss: 218.847
iter:200 | loss: 113.845
iter:300 | loss: 201.276
iter:400 | loss: 211.801
iter:500 | loss: 151.521
iter:600 | loss: 265.504
iter:700 | loss: 181.710
iter:800 | loss: 171.422
iter:900 | loss: 121.704
iter:1000 | loss: 181.837
iter:1100 | loss: 202.630
iter:1200 | loss: 128.334
iter:1300 | loss: 152.444
iter:1400 | loss: 203.198
iter:1500 | loss: 179.289
iter:1600 | loss: 133.116
iter:1700 | loss: 110.056
iter:1800 | loss: 92.800
iter:1900 | loss: 137.848
iter:2000 | loss: 165.418
iter:2100 | loss: 129.533
iter:2200 | loss: 122.437
iter:2300 | loss: 153.504
iter:2400 | loss: 106.006
iter:2500 | loss: 122.949
iter:2600 | loss: 125.103
iter:2700 | loss: 166.610
iter:2800 | loss: 137.376
iter:2900 | loss: 135.740
iter:3000 | loss: 108.622
iter:3100 | loss: 167.471
iter:3200 | loss: 126.842
iter:3300 | loss: 114.519
iter:3400 | loss: 244.040
iter:3500 | loss: 141.698
iter:3600 | loss: 146.400
iter:3700 | loss: 134.800
iter:3800 | loss: 189.908
iter:3900 | loss: 108.843
| end of epoch   8 | time: 859.68s | train_loss 156.7145 | valid rse 0.0543 | valid rae 0.0386 | valid corr  0.9349
iter:  0 | loss: 168.904
iter:100 | loss: 160.109
iter:200 | loss: 167.373
iter:300 | loss: 111.474
iter:400 | loss: 131.274
iter:500 | loss: 207.051
iter:600 | loss: 139.361
iter:700 | loss: 108.938
iter:800 | loss: 103.429
iter:900 | loss: 126.927
iter:1000 | loss: 151.777
iter:1100 | loss: 212.901
iter:1200 | loss: 109.844
iter:1300 | loss: 195.687
iter:1400 | loss: 176.072
iter:1500 | loss: 200.852
iter:1600 | loss: 254.097
iter:1700 | loss: 147.737
iter:1800 | loss: 215.486
iter:1900 | loss: 196.533
iter:2000 | loss: 167.079
iter:2100 | loss: 161.230
iter:2200 | loss: 210.103
iter:2300 | loss: 184.727
iter:2400 | loss: 182.924
iter:2500 | loss: 115.369
iter:2600 | loss: 139.279
iter:2700 | loss: 183.051
iter:2800 | loss: 156.195
iter:2900 | loss: 113.216
iter:3000 | loss: 272.870
iter:3100 | loss: 197.724
iter:3200 | loss: 123.834
iter:3300 | loss: 124.985
iter:3400 | loss: 223.080
iter:3500 | loss: 136.455
iter:3600 | loss: 159.975
iter:3700 | loss: 211.201
iter:3800 | loss: 165.119
iter:3900 | loss: 162.446
| end of epoch   9 | time: 858.74s | train_loss 155.7996 | valid rse 0.0522 | valid rae 0.0372 | valid corr  0.9347
iter:  0 | loss: 173.236
iter:100 | loss: 95.505
iter:200 | loss: 136.465
iter:300 | loss: 117.002
iter:400 | loss: 169.506
iter:500 | loss: 107.139
iter:600 | loss: 111.192
iter:700 | loss: 143.854
iter:800 | loss: 145.573
iter:900 | loss: 101.340
iter:1000 | loss: 138.163
iter:1100 | loss: 131.551
iter:1200 | loss: 89.292
iter:1300 | loss: 164.549
iter:1400 | loss: 212.040
iter:1500 | loss: 136.652
iter:1600 | loss: 168.702
iter:1700 | loss: 297.886
iter:1800 | loss: 110.763
iter:1900 | loss: 184.922
iter:2000 | loss: 154.625
iter:2100 | loss: 209.077
iter:2200 | loss: 153.221
iter:2300 | loss: 209.765
iter:2400 | loss: 162.888
iter:2500 | loss: 112.355
iter:2600 | loss: 202.294
iter:2700 | loss: 152.123
iter:2800 | loss: 126.429
iter:2900 | loss: 197.596
iter:3000 | loss: 192.820
iter:3100 | loss: 147.249
iter:3200 | loss: 130.919
iter:3300 | loss: 177.193
iter:3400 | loss: 198.364
iter:3500 | loss: 134.302
iter:3600 | loss: 168.738
iter:3700 | loss: 158.552
iter:3800 | loss: 149.419
iter:3900 | loss: 109.619
| end of epoch  10 | time: 858.96s | train_loss 154.6516 | valid rse 0.0515 | valid rae 0.0364 | valid corr  0.9354
test rse 0.0784 | test rae 0.0436 | test corr 0.9426
iter:  0 | loss: 140.048
iter:100 | loss: 126.539
iter:200 | loss: 280.976
iter:300 | loss: 181.714
iter:400 | loss: 136.549
iter:500 | loss: 153.219
iter:600 | loss: 141.027
iter:700 | loss: 104.765
iter:800 | loss: 128.438
iter:900 | loss: 122.917
iter:1000 | loss: 129.745
iter:1100 | loss: 144.460
iter:1200 | loss: 151.492
iter:1300 | loss: 137.347
iter:1400 | loss: 160.178
iter:1500 | loss: 123.908
iter:1600 | loss: 115.033
iter:1700 | loss: 163.275
iter:1800 | loss: 119.128
iter:1900 | loss: 218.580
iter:2000 | loss: 142.161
iter:2100 | loss: 161.714
iter:2200 | loss: 153.187
iter:2300 | loss: 171.645
iter:2400 | loss: 169.076
iter:2500 | loss: 189.540
iter:2600 | loss: 174.866
iter:2700 | loss: 134.536
iter:2800 | loss: 162.721
iter:2900 | loss: 153.502
iter:3000 | loss: 200.447
iter:3100 | loss: 116.825
iter:3200 | loss: 248.088
iter:3300 | loss: 186.668
iter:3400 | loss: 284.565
iter:3500 | loss: 117.060
iter:3600 | loss: 169.696
iter:3700 | loss: 209.700
iter:3800 | loss: 138.331
iter:3900 | loss: 219.294
| end of epoch  11 | time: 859.70s | train_loss 153.8303 | valid rse 0.0533 | valid rae 0.0370 | valid corr  0.9353
iter:  0 | loss: 146.333
iter:100 | loss: 176.449
iter:200 | loss: 116.371
iter:300 | loss: 100.381
iter:400 | loss: 160.388
iter:500 | loss: 125.065
iter:600 | loss: 160.288
iter:700 | loss: 122.869
iter:800 | loss: 150.339
iter:900 | loss: 133.446
iter:1000 | loss: 170.868
iter:1100 | loss: 137.691
iter:1200 | loss: 112.555
iter:1300 | loss: 141.969
iter:1400 | loss: 171.090
iter:1500 | loss: 128.459
iter:1600 | loss: 284.014
iter:1700 | loss: 159.471
iter:1800 | loss: 161.398
iter:1900 | loss: 161.488
iter:2000 | loss: 157.672
iter:2100 | loss: 118.148
iter:2200 | loss: 129.650
iter:2300 | loss: 158.679
iter:2400 | loss: 169.055
iter:2500 | loss: 168.353
iter:2600 | loss: 132.919
iter:2700 | loss: 164.632
iter:2800 | loss: 126.715
iter:2900 | loss: 179.242
iter:3000 | loss: 125.848
iter:3100 | loss: 138.174
iter:3200 | loss: 235.886
iter:3300 | loss: 167.015
iter:3400 | loss: 135.519
iter:3500 | loss: 130.416
iter:3600 | loss: 149.619
iter:3700 | loss: 162.330
iter:3800 | loss: 124.140
iter:3900 | loss: 201.876
| end of epoch  12 | time: 858.78s | train_loss 152.7237 | valid rse 0.0528 | valid rae 0.0369 | valid corr  0.9374
iter:  0 | loss: 113.569
iter:100 | loss: 102.084
iter:200 | loss: 153.181
iter:300 | loss: 139.038
iter:400 | loss: 169.093
iter:500 | loss: 166.242
iter:600 | loss: 171.536
iter:700 | loss: 195.434
iter:800 | loss: 159.809
iter:900 | loss: 193.030
iter:1000 | loss: 139.418
iter:1100 | loss: 132.299
iter:1200 | loss: 149.215
iter:1300 | loss: 129.560
iter:1400 | loss: 161.245
iter:1500 | loss: 150.430
iter:1600 | loss: 123.803
iter:1700 | loss: 152.823
iter:1800 | loss: 114.837
iter:1900 | loss: 139.282
iter:2000 | loss: 123.760
iter:2100 | loss: 324.664
iter:2200 | loss: 128.314
iter:2300 | loss: 99.675
iter:2400 | loss: 161.944
iter:2500 | loss: 146.598
iter:2600 | loss: 160.563
iter:2700 | loss: 147.441
iter:2800 | loss: 110.608
iter:2900 | loss: 174.900
iter:3000 | loss: 182.916
iter:3100 | loss: 172.226
iter:3200 | loss: 87.882
iter:3300 | loss: 175.745
iter:3400 | loss: 153.248
iter:3500 | loss: 171.846
iter:3600 | loss: 125.491
iter:3700 | loss: 145.605
iter:3800 | loss: 215.937
iter:3900 | loss: 172.807
| end of epoch  13 | time: 859.34s | train_loss 152.2863 | valid rse 0.0512 | valid rae 0.0364 | valid corr  0.9363
iter:  0 | loss: 127.995
iter:100 | loss: 176.064
iter:200 | loss: 171.448
iter:300 | loss: 232.257
iter:400 | loss: 215.784
iter:500 | loss: 187.684
iter:600 | loss: 146.561
iter:700 | loss: 152.715
iter:800 | loss: 165.604
iter:900 | loss: 152.449
iter:1000 | loss: 149.061
iter:1100 | loss: 289.526
iter:1200 | loss: 136.137
iter:1300 | loss: 171.884
iter:1400 | loss: 130.257
iter:1500 | loss: 182.387
iter:1600 | loss: 158.391
iter:1700 | loss: 110.979
iter:1800 | loss: 105.926
iter:1900 | loss: 240.863
iter:2000 | loss: 101.496
iter:2100 | loss: 233.227
iter:2200 | loss: 151.326
iter:2300 | loss: 159.579
iter:2400 | loss: 176.212
iter:2500 | loss: 195.393
iter:2600 | loss: 239.362
iter:2700 | loss: 148.614
iter:2800 | loss: 128.490
iter:2900 | loss: 113.059
iter:3000 | loss: 136.855
iter:3100 | loss: 200.597
iter:3200 | loss: 109.551
iter:3300 | loss: 184.257
iter:3400 | loss: 156.775
iter:3500 | loss: 155.873
iter:3600 | loss: 255.273
iter:3700 | loss: 94.899
iter:3800 | loss: 177.397
iter:3900 | loss: 150.201
| end of epoch  14 | time: 860.11s | train_loss 151.8083 | valid rse 0.0516 | valid rae 0.0365 | valid corr  0.9366
iter:  0 | loss: 151.382
iter:100 | loss: 212.207
iter:200 | loss: 125.667
iter:300 | loss: 250.186
iter:400 | loss: 140.939
iter:500 | loss: 146.268
iter:600 | loss: 146.970
iter:700 | loss: 121.282
iter:800 | loss: 253.646
iter:900 | loss: 143.587
iter:1000 | loss: 169.415
iter:1100 | loss: 166.107
iter:1200 | loss: 151.364
iter:1300 | loss: 115.895
iter:1400 | loss: 166.162
iter:1500 | loss: 207.485
iter:1600 | loss: 202.174
iter:1700 | loss: 121.167
iter:1800 | loss: 134.303
iter:1900 | loss: 186.231
iter:2000 | loss: 141.294
iter:2100 | loss: 114.006
iter:2200 | loss: 134.880
iter:2300 | loss: 98.689
iter:2400 | loss: 153.406
iter:2500 | loss: 152.109
iter:2600 | loss: 150.330
iter:2700 | loss: 136.327
iter:2800 | loss: 165.485
iter:2900 | loss: 169.660
iter:3000 | loss: 109.574
iter:3100 | loss: 156.746
iter:3200 | loss: 159.081
iter:3300 | loss: 131.533
iter:3400 | loss: 142.387
iter:3500 | loss: 151.370
iter:3600 | loss: 128.039
iter:3700 | loss: 123.522
iter:3800 | loss: 138.989
iter:3900 | loss: 144.792
| end of epoch  15 | time: 858.37s | train_loss 151.2209 | valid rse 0.0507 | valid rae 0.0362 | valid corr  0.9368
test rse 0.0765 | test rae 0.0430 | test corr 0.9433
iter:  0 | loss: 201.395
iter:100 | loss: 215.354
iter:200 | loss: 246.210
iter:300 | loss: 147.040
iter:400 | loss: 134.518
iter:500 | loss: 118.664
iter:600 | loss: 162.937
iter:700 | loss: 151.674
iter:800 | loss: 117.529
iter:900 | loss: 146.464
iter:1000 | loss: 119.270
iter:1100 | loss: 222.438
iter:1200 | loss: 163.892
iter:1300 | loss: 178.089
iter:1400 | loss: 186.728
iter:1500 | loss: 131.676
iter:1600 | loss: 141.481
iter:1700 | loss: 105.957
iter:1800 | loss: 123.463
iter:1900 | loss: 262.665
iter:2000 | loss: 162.167
iter:2100 | loss: 108.204
iter:2200 | loss: 148.287
iter:2300 | loss: 110.499
iter:2400 | loss: 121.928
iter:2500 | loss: 94.198
iter:2600 | loss: 162.001
iter:2700 | loss: 137.310
iter:2800 | loss: 196.043
iter:2900 | loss: 110.008
iter:3000 | loss: 163.554
iter:3100 | loss: 120.456
iter:3200 | loss: 117.586
iter:3300 | loss: 205.595
iter:3400 | loss: 223.770
iter:3500 | loss: 142.484
iter:3600 | loss: 174.388
iter:3700 | loss: 154.207
iter:3800 | loss: 206.521
iter:3900 | loss: 118.464
| end of epoch  16 | time: 858.65s | train_loss 150.2888 | valid rse 0.0510 | valid rae 0.0361 | valid corr  0.9366
iter:  0 | loss: 107.293
iter:100 | loss: 143.256
iter:200 | loss: 219.081
iter:300 | loss: 111.942
iter:400 | loss: 134.518
iter:500 | loss: 112.826
iter:600 | loss: 117.346
iter:700 | loss: 131.647
iter:800 | loss: 166.352
iter:900 | loss: 140.744
iter:1000 | loss: 133.646
iter:1100 | loss: 206.773
iter:1200 | loss: 262.090
iter:1300 | loss: 131.469
iter:1400 | loss: 188.340
iter:1500 | loss: 163.886
iter:1600 | loss: 173.332
iter:1700 | loss: 182.595
iter:1800 | loss: 160.797
iter:1900 | loss: 286.670
iter:2000 | loss: 128.828
iter:2100 | loss: 117.508
iter:2200 | loss: 156.665
iter:2300 | loss: 195.174
iter:2400 | loss: 135.889
iter:2500 | loss: 87.415
iter:2600 | loss: 169.606
iter:2700 | loss: 169.175
iter:2800 | loss: 301.073
iter:2900 | loss: 123.725
iter:3000 | loss: 136.549
iter:3100 | loss: 125.220
iter:3200 | loss: 138.247
iter:3300 | loss: 129.366
iter:3400 | loss: 199.780
iter:3500 | loss: 103.375
iter:3600 | loss: 110.734
iter:3700 | loss: 122.750
iter:3800 | loss: 108.052
iter:3900 | loss: 173.338
| end of epoch  17 | time: 859.86s | train_loss 149.7471 | valid rse 0.0544 | valid rae 0.0385 | valid corr  0.9384
iter:  0 | loss: 177.403
iter:100 | loss: 147.039
iter:200 | loss: 218.105
iter:300 | loss: 159.989
iter:400 | loss: 107.702
iter:500 | loss: 234.535
iter:600 | loss: 148.485
iter:700 | loss: 115.859
iter:800 | loss: 134.342
iter:900 | loss: 127.459
iter:1000 | loss: 98.592
iter:1100 | loss: 183.807
iter:1200 | loss: 120.141
iter:1300 | loss: 153.486
iter:1400 | loss: 166.168
iter:1500 | loss: 166.761
iter:1600 | loss: 133.499
iter:1700 | loss: 122.461
iter:1800 | loss: 208.537
iter:1900 | loss: 110.841
iter:2000 | loss: 130.437
iter:2100 | loss: 209.589
iter:2200 | loss: 219.083
iter:2300 | loss: 103.103
iter:2400 | loss: 128.378
iter:2500 | loss: 149.802
iter:2600 | loss: 104.350
iter:2700 | loss: 184.449
iter:2800 | loss: 168.655
iter:2900 | loss: 157.438
iter:3000 | loss: 109.535
iter:3100 | loss: 194.386
iter:3200 | loss: 177.056
iter:3300 | loss: 112.181
iter:3400 | loss: 206.870
iter:3500 | loss: 204.677
iter:3600 | loss: 148.429
iter:3700 | loss: 166.061
iter:3800 | loss: 212.923
iter:3900 | loss: 247.859
| end of epoch  18 | time: 860.23s | train_loss 149.0669 | valid rse 0.0529 | valid rae 0.0388 | valid corr  0.9372
iter:  0 | loss: 153.908
iter:100 | loss: 127.974
iter:200 | loss: 102.388
iter:300 | loss: 167.551
iter:400 | loss: 138.174
iter:500 | loss: 120.264
iter:600 | loss: 162.402
iter:700 | loss: 149.037
iter:800 | loss: 110.437
iter:900 | loss: 182.756
iter:1000 | loss: 110.329
iter:1100 | loss: 198.017
iter:1200 | loss: 146.697
iter:1300 | loss: 170.646
iter:1400 | loss: 160.923
iter:1500 | loss: 124.715
iter:1600 | loss: 155.632
iter:1700 | loss: 192.222
iter:1800 | loss: 211.294
iter:1900 | loss: 145.854
iter:2000 | loss: 145.932
iter:2100 | loss: 106.511
iter:2200 | loss: 131.698
iter:2300 | loss: 161.436
iter:2400 | loss: 141.753
iter:2500 | loss: 140.044
iter:2600 | loss: 119.086
iter:2700 | loss: 112.509
iter:2800 | loss: 140.049
iter:2900 | loss: 163.057
iter:3000 | loss: 110.141
iter:3100 | loss: 143.167
iter:3200 | loss: 153.303
iter:3300 | loss: 233.887
iter:3400 | loss: 178.850
iter:3500 | loss: 140.468
iter:3600 | loss: 151.181
iter:3700 | loss: 125.527
iter:3800 | loss: 127.639
iter:3900 | loss: 164.067
| end of epoch  19 | time: 859.55s | train_loss 148.6814 | valid rse 0.0506 | valid rae 0.0358 | valid corr  0.9385
iter:  0 | loss: 133.085
iter:100 | loss: 118.127
iter:200 | loss: 130.750
iter:300 | loss: 166.222
iter:400 | loss: 140.530
iter:500 | loss: 275.495
iter:600 | loss: 150.316
iter:700 | loss: 209.337
iter:800 | loss: 111.810
iter:900 | loss: 123.493
iter:1000 | loss: 165.506
iter:1100 | loss: 140.072
iter:1200 | loss: 151.812
iter:1300 | loss: 141.012
iter:1400 | loss: 129.952
iter:1500 | loss: 112.144
iter:1600 | loss: 164.386
iter:1700 | loss: 138.707
iter:1800 | loss: 162.004
iter:1900 | loss: 142.184
iter:2000 | loss: 117.397
iter:2100 | loss: 117.864
iter:2200 | loss: 126.998
iter:2300 | loss: 141.608
iter:2400 | loss: 152.270
iter:2500 | loss: 101.547
iter:2600 | loss: 121.985
iter:2700 | loss: 126.299
iter:2800 | loss: 122.547
iter:2900 | loss: 132.964
iter:3000 | loss: 111.310
iter:3100 | loss: 131.954
iter:3200 | loss: 117.627
iter:3300 | loss: 149.400
iter:3400 | loss: 168.643
iter:3500 | loss: 107.736
iter:3600 | loss: 167.351
iter:3700 | loss: 179.830
iter:3800 | loss: 116.136
iter:3900 | loss: 141.491
| end of epoch  20 | time: 859.79s | train_loss 148.6451 | valid rse 0.0510 | valid rae 0.0361 | valid corr  0.9388
test rse 0.0765 | test rae 0.0431 | test corr 0.9451
iter:  0 | loss: 133.422
iter:100 | loss: 149.267
iter:200 | loss: 145.519
iter:300 | loss: 251.021
iter:400 | loss: 238.800
iter:500 | loss: 124.679
iter:600 | loss: 111.967
iter:700 | loss: 94.971
iter:800 | loss: 127.085
iter:900 | loss: 84.672
iter:1000 | loss: 87.720
iter:1100 | loss: 179.106
iter:1200 | loss: 119.376
iter:1300 | loss: 108.203
iter:1400 | loss: 142.118
iter:1500 | loss: 116.822
iter:1600 | loss: 196.634
iter:1700 | loss: 126.153
iter:1800 | loss: 126.234
iter:1900 | loss: 193.117
iter:2000 | loss: 170.158
iter:2100 | loss: 199.581
iter:2200 | loss: 105.055
iter:2300 | loss: 134.097
iter:2400 | loss: 121.749
iter:2500 | loss: 129.226
iter:2600 | loss: 120.060
iter:2700 | loss: 93.375
iter:2800 | loss: 176.521
iter:2900 | loss: 168.999
iter:3000 | loss: 143.062
iter:3100 | loss: 95.677
iter:3200 | loss: 123.364
iter:3300 | loss: 111.667
iter:3400 | loss: 141.254
iter:3500 | loss: 241.645
iter:3600 | loss: 109.728
iter:3700 | loss: 144.219
iter:3800 | loss: 195.754
iter:3900 | loss: 169.032
| end of epoch  21 | time: 859.34s | train_loss 148.0090 | valid rse 0.0535 | valid rae 0.0390 | valid corr  0.9387
iter:  0 | loss: 133.486
iter:100 | loss: 170.619
iter:200 | loss: 81.549
iter:300 | loss: 199.166
iter:400 | loss: 117.394
iter:500 | loss: 137.157
iter:600 | loss: 115.346
iter:700 | loss: 129.577
iter:800 | loss: 198.752
iter:900 | loss: 114.574
iter:1000 | loss: 126.807
iter:1100 | loss: 103.388
iter:1200 | loss: 125.513
iter:1300 | loss: 121.988
iter:1400 | loss: 111.845
iter:1500 | loss: 102.373
iter:1600 | loss: 113.559
iter:1700 | loss: 131.679
iter:1800 | loss: 91.983
iter:1900 | loss: 177.486
iter:2000 | loss: 108.001
iter:2100 | loss: 121.756
iter:2200 | loss: 111.918
iter:2300 | loss: 207.471
iter:2400 | loss: 130.671
iter:2500 | loss: 204.660
iter:2600 | loss: 122.953
iter:2700 | loss: 139.658
iter:2800 | loss: 91.924
iter:2900 | loss: 199.873
iter:3000 | loss: 94.401
iter:3100 | loss: 124.216
iter:3200 | loss: 176.418
iter:3300 | loss: 141.392
iter:3400 | loss: 163.339
iter:3500 | loss: 134.193
iter:3600 | loss: 83.658
iter:3700 | loss: 132.769
iter:3800 | loss: 149.828
iter:3900 | loss: 177.870
| end of epoch  22 | time: 859.34s | train_loss 147.2614 | valid rse 0.0508 | valid rae 0.0356 | valid corr  0.9373
iter:  0 | loss: 124.515
iter:100 | loss: 170.342
iter:200 | loss: 122.800
iter:300 | loss: 114.352
iter:400 | loss: 157.715
iter:500 | loss: 135.861
iter:600 | loss: 149.721
iter:700 | loss: 207.662
iter:800 | loss: 121.922
iter:900 | loss: 104.198
iter:1000 | loss: 152.544
iter:1100 | loss: 130.193
iter:1200 | loss: 238.536
iter:1300 | loss: 100.350
iter:1400 | loss: 145.448
iter:1500 | loss: 188.900
iter:1600 | loss: 235.363
iter:1700 | loss: 190.171
iter:1800 | loss: 134.915
iter:1900 | loss: 157.509
iter:2000 | loss: 174.908
iter:2100 | loss: 119.467
iter:2200 | loss: 159.825
iter:2300 | loss: 158.309
iter:2400 | loss: 148.687
iter:2500 | loss: 192.820
iter:2600 | loss: 137.413
iter:2700 | loss: 174.044
iter:2800 | loss: 122.851
iter:2900 | loss: 116.363
iter:3000 | loss: 124.656
iter:3100 | loss: 113.576
iter:3200 | loss: 133.865
iter:3300 | loss: 160.489
iter:3400 | loss: 159.659
iter:3500 | loss: 108.149
iter:3600 | loss: 97.943
iter:3700 | loss: 128.098
iter:3800 | loss: 198.458
iter:3900 | loss: 151.830
| end of epoch  23 | time: 858.80s | train_loss 147.2381 | valid rse 0.0509 | valid rae 0.0371 | valid corr  0.9390
iter:  0 | loss: 126.625
iter:100 | loss: 266.381
iter:200 | loss: 120.564
iter:300 | loss: 128.365
iter:400 | loss: 163.701
iter:500 | loss: 241.618
iter:600 | loss: 103.523
iter:700 | loss: 156.010
iter:800 | loss: 101.993
iter:900 | loss: 107.902
iter:1000 | loss: 145.671
iter:1100 | loss: 128.569
iter:1200 | loss: 133.139
iter:1300 | loss: 198.339
iter:1400 | loss: 206.041
iter:1500 | loss: 137.206
iter:1600 | loss: 136.470
iter:1700 | loss: 94.191
iter:1800 | loss: 157.022
iter:1900 | loss: 105.167
iter:2000 | loss: 150.888
iter:2100 | loss: 191.537
iter:2200 | loss: 117.866
iter:2300 | loss: 165.706
iter:2400 | loss: 99.368
iter:2500 | loss: 144.237
iter:2600 | loss: 133.170
iter:2700 | loss: 155.323
iter:2800 | loss: 139.965
iter:2900 | loss: 151.500
iter:3000 | loss: 176.488
iter:3100 | loss: 206.227
iter:3200 | loss: 228.872
iter:3300 | loss: 174.863
iter:3400 | loss: 130.922
iter:3500 | loss: 232.827
iter:3600 | loss: 105.098
iter:3700 | loss: 117.019
iter:3800 | loss: 106.284
iter:3900 | loss: 248.184
| end of epoch  24 | time: 859.08s | train_loss 146.6037 | valid rse 0.0518 | valid rae 0.0370 | valid corr  0.9401
iter:  0 | loss: 130.779
iter:100 | loss: 217.852
iter:200 | loss: 91.327
iter:300 | loss: 106.092
iter:400 | loss: 218.244
iter:500 | loss: 145.049
iter:600 | loss: 108.508
iter:700 | loss: 226.515
iter:800 | loss: 140.920
iter:900 | loss: 246.151
iter:1000 | loss: 108.248
iter:1100 | loss: 137.358
iter:1200 | loss: 90.748
iter:1300 | loss: 129.059
iter:1400 | loss: 123.087
iter:1500 | loss: 128.297
iter:1600 | loss: 129.625
iter:1700 | loss: 202.629
iter:1800 | loss: 121.123
iter:1900 | loss: 148.953
iter:2000 | loss: 118.890
iter:2100 | loss: 153.941
iter:2200 | loss: 118.179
iter:2300 | loss: 162.351
iter:2400 | loss: 104.321
iter:2500 | loss: 201.583
iter:2600 | loss: 163.431
iter:2700 | loss: 173.466
iter:2800 | loss: 126.992
iter:2900 | loss: 101.062
iter:3000 | loss: 134.898
iter:3100 | loss: 130.331
iter:3200 | loss: 111.812
iter:3300 | loss: 185.721
iter:3400 | loss: 116.329
iter:3500 | loss: 152.690
iter:3600 | loss: 192.184
iter:3700 | loss: 179.695
iter:3800 | loss: 95.316
iter:3900 | loss: 172.824
| end of epoch  25 | time: 860.24s | train_loss 146.2515 | valid rse 0.0517 | valid rae 0.0377 | valid corr  0.9393
test rse 0.0776 | test rae 0.0452 | test corr 0.9464
iter:  0 | loss: 189.954
iter:100 | loss: 99.578
iter:200 | loss: 201.478
iter:300 | loss: 147.217
iter:400 | loss: 191.981
iter:500 | loss: 107.090
iter:600 | loss: 200.359
iter:700 | loss: 134.019
iter:800 | loss: 137.046
iter:900 | loss: 159.932
iter:1000 | loss: 167.363
iter:1100 | loss: 142.099
iter:1200 | loss: 171.545
iter:1300 | loss: 101.124
iter:1400 | loss: 125.057
iter:1500 | loss: 183.366
iter:1600 | loss: 134.822
iter:1700 | loss: 137.531
iter:1800 | loss: 233.677
iter:1900 | loss: 125.111
iter:2000 | loss: 146.202
iter:2100 | loss: 174.786
iter:2200 | loss: 216.635
iter:2300 | loss: 152.834
iter:2400 | loss: 206.032
iter:2500 | loss: 94.029
iter:2600 | loss: 138.387
iter:2700 | loss: 139.012
iter:2800 | loss: 102.742
iter:2900 | loss: 172.362
iter:3000 | loss: 213.502
iter:3100 | loss: 178.781
iter:3200 | loss: 136.993
iter:3300 | loss: 165.128
iter:3400 | loss: 111.000
iter:3500 | loss: 131.710
iter:3600 | loss: 114.560
iter:3700 | loss: 115.504
iter:3800 | loss: 124.413
iter:3900 | loss: 140.471
| end of epoch  26 | time: 857.86s | train_loss 145.1584 | valid rse 0.0500 | valid rae 0.0349 | valid corr  0.9387
iter:  0 | loss: 182.597
iter:100 | loss: 147.054
iter:200 | loss: 121.247
iter:300 | loss: 228.276
iter:400 | loss: 112.270
iter:500 | loss: 147.701
iter:600 | loss: 118.538
iter:700 | loss: 145.907
iter:800 | loss: 169.595
iter:900 | loss: 96.736
iter:1000 | loss: 141.083
iter:1100 | loss: 118.307
iter:1200 | loss: 94.525
iter:1300 | loss: 126.367
iter:1400 | loss: 89.037
iter:1500 | loss: 116.366
iter:1600 | loss: 196.677
iter:1700 | loss: 123.896
iter:1800 | loss: 116.611
iter:1900 | loss: 125.225
iter:2000 | loss: 107.494
iter:2100 | loss: 140.828
iter:2200 | loss: 171.178
iter:2300 | loss: 155.982
iter:2400 | loss: 173.958
iter:2500 | loss: 151.590
iter:2600 | loss: 246.221
iter:2700 | loss: 99.822
iter:2800 | loss: 150.622
iter:2900 | loss: 174.944
iter:3000 | loss: 136.124
iter:3100 | loss: 86.959
iter:3200 | loss: 104.780
iter:3300 | loss: 119.952
iter:3400 | loss: 121.275
iter:3500 | loss: 110.047
iter:3600 | loss: 128.612
iter:3700 | loss: 118.142
iter:3800 | loss: 162.986
iter:3900 | loss: 147.387
| end of epoch  27 | time: 858.37s | train_loss 145.3046 | valid rse 0.0496 | valid rae 0.0347 | valid corr  0.9395
iter:  0 | loss: 117.255
iter:100 | loss: 137.985
iter:200 | loss: 116.334
iter:300 | loss: 95.653
iter:400 | loss: 113.187
iter:500 | loss: 140.877
iter:600 | loss: 133.416
iter:700 | loss: 132.266
iter:800 | loss: 140.183
iter:900 | loss: 230.156
iter:1000 | loss: 136.174
iter:1100 | loss: 169.864
iter:1200 | loss: 145.966
iter:1300 | loss: 120.726
iter:1400 | loss: 210.545
iter:1500 | loss: 116.063
iter:1600 | loss: 148.225
iter:1700 | loss: 140.072
iter:1800 | loss: 147.450
iter:1900 | loss: 113.142
iter:2000 | loss: 170.650
iter:2100 | loss: 131.549
iter:2200 | loss: 126.838
iter:2300 | loss: 152.477
iter:2400 | loss: 218.120
iter:2500 | loss: 144.012
iter:2600 | loss: 234.760
iter:2700 | loss: 126.206
iter:2800 | loss: 103.924
iter:2900 | loss: 140.813
iter:3000 | loss: 90.542
iter:3100 | loss: 150.181
iter:3200 | loss: 111.542
iter:3300 | loss: 128.256
iter:3400 | loss: 135.772
iter:3500 | loss: 142.044
iter:3600 | loss: 165.538
iter:3700 | loss: 117.787
iter:3800 | loss: 119.961
iter:3900 | loss: 114.742
| end of epoch  28 | time: 859.29s | train_loss 144.3736 | valid rse 0.0500 | valid rae 0.0350 | valid corr  0.9390
iter:  0 | loss: 115.811
iter:100 | loss: 125.884
iter:200 | loss: 89.744
iter:300 | loss: 158.191
iter:400 | loss: 140.308
iter:500 | loss: 151.541
iter:600 | loss: 170.319
iter:700 | loss: 115.839
iter:800 | loss: 244.253
iter:900 | loss: 98.741
iter:1000 | loss: 155.105
iter:1100 | loss: 151.413
iter:1200 | loss: 112.311
iter:1300 | loss: 135.864
iter:1400 | loss: 215.884
iter:1500 | loss: 146.757
iter:1600 | loss: 127.674
iter:1700 | loss: 186.073
iter:1800 | loss: 251.798
iter:1900 | loss: 152.069
iter:2000 | loss: 119.792
iter:2100 | loss: 135.023
iter:2200 | loss: 115.299
iter:2300 | loss: 123.270
iter:2400 | loss: 255.752
iter:2500 | loss: 155.980
iter:2600 | loss: 113.019
iter:2700 | loss: 190.893
iter:2800 | loss: 163.238
iter:2900 | loss: 156.963
iter:3000 | loss: 105.001
iter:3100 | loss: 151.392
iter:3200 | loss: 187.725
iter:3300 | loss: 158.515
iter:3400 | loss: 171.310
iter:3500 | loss: 132.892
iter:3600 | loss: 157.072
iter:3700 | loss: 131.810
iter:3800 | loss: 96.087
iter:3900 | loss: 136.795
| end of epoch  29 | time: 858.42s | train_loss 143.9136 | valid rse 0.0513 | valid rae 0.0364 | valid corr  0.9406
iter:  0 | loss: 190.198
iter:100 | loss: 112.953
iter:200 | loss: 137.064
iter:300 | loss: 104.113
iter:400 | loss: 92.006
iter:500 | loss: 199.191
iter:600 | loss: 122.577
iter:700 | loss: 162.638
iter:800 | loss: 91.907
iter:900 | loss: 137.865
iter:1000 | loss: 113.415
iter:1100 | loss: 156.148
iter:1200 | loss: 227.884
iter:1300 | loss: 105.443
iter:1400 | loss: 171.743
iter:1500 | loss: 139.110
iter:1600 | loss: 112.746
iter:1700 | loss: 119.382
iter:1800 | loss: 92.540
iter:1900 | loss: 123.876
iter:2000 | loss: 154.016
iter:2100 | loss: 126.549
iter:2200 | loss: 179.112
iter:2300 | loss: 89.752
iter:2400 | loss: 158.705
iter:2500 | loss: 142.575
iter:2600 | loss: 213.206
iter:2700 | loss: 119.253
iter:2800 | loss: 158.975
iter:2900 | loss: 139.265
iter:3000 | loss: 166.539
iter:3100 | loss: 197.482
iter:3200 | loss: 166.581
iter:3300 | loss: 105.914
iter:3400 | loss: 187.649
iter:3500 | loss: 157.953
iter:3600 | loss: 188.216
iter:3700 | loss: 95.253
iter:3800 | loss: 164.259
iter:3900 | loss: 114.865
| end of epoch  30 | time: 858.30s | train_loss 143.9200 | valid rse 0.0497 | valid rae 0.0348 | valid corr  0.9403
test rse 0.0760 | test rae 0.0420 | test corr 0.9468
final test rse 0.0751 | test rae 0.0414 | test corr 0.9465
Namespace(L1Loss=True, batch_size=4, buildA_true=True, clip=5, conv_channels=16, data='./data/electricity.txt', device='cuda:0', dilation_exponential=2, dropout=0.3, end_channels=64, epochs=30, gcn_depth=2, gcn_true=True, horizon=3, in_dim=1, layers=5, log_interval=2000, lr=0.0001, node_dim=40, normalize=2, num_nodes=321, num_split=1, optim='adam', propalpha=0.05, residual_channels=16, save='./models/model-electricity-3.pt', seq_in_len=168, seq_out_len=1, skip_channels=32, step_size=100, subgraph_size=20, tanhalpha=3, weight_decay=1e-05)
The recpetive field size is 187
Number of model parameters is 362385
begin training
iter:  0 | loss: 2929.628
iter:100 | loss: 336.098
iter:200 | loss: 265.377
iter:300 | loss: 374.393
iter:400 | loss: 296.465
iter:500 | loss: 258.848
iter:600 | loss: 202.608
iter:700 | loss: 215.389
iter:800 | loss: 260.869
iter:900 | loss: 308.272
iter:1000 | loss: 139.216
iter:1100 | loss: 270.756
iter:1200 | loss: 186.912
iter:1300 | loss: 220.051
iter:1400 | loss: 338.815
iter:1500 | loss: 166.849
iter:1600 | loss: 225.600
iter:1700 | loss: 247.135
iter:1800 | loss: 183.064
iter:1900 | loss: 175.214
iter:2000 | loss: 311.007
iter:2100 | loss: 263.910
iter:2200 | loss: 238.049
iter:2300 | loss: 195.508
iter:2400 | loss: 205.131
iter:2500 | loss: 295.124
iter:2600 | loss: 175.185
iter:2700 | loss: 180.126
iter:2800 | loss: 174.424
iter:2900 | loss: 200.993
iter:3000 | loss: 211.913
iter:3100 | loss: 182.961
iter:3200 | loss: 152.913
iter:3300 | loss: 281.695
iter:3400 | loss: 142.246
iter:3500 | loss: 239.603
iter:3600 | loss: 157.739
iter:3700 | loss: 219.364
iter:3800 | loss: 119.702
iter:3900 | loss: 160.793
| end of epoch   1 | time: 857.03s | train_loss 235.0000 | valid rse 0.0575 | valid rae 0.0415 | valid corr  0.9232
iter:  0 | loss: 164.247
iter:100 | loss: 241.246
iter:200 | loss: 161.031
iter:300 | loss: 227.626
iter:400 | loss: 213.416
iter:500 | loss: 188.802
iter:600 | loss: 156.536
iter:700 | loss: 160.200
iter:800 | loss: 143.940
iter:900 | loss: 123.036
iter:1000 | loss: 119.431
iter:1100 | loss: 210.811
iter:1200 | loss: 134.935
iter:1300 | loss: 146.820
iter:1400 | loss: 146.481
iter:1500 | loss: 218.789
iter:1600 | loss: 236.095
iter:1700 | loss: 116.849
iter:1800 | loss: 137.220
iter:1900 | loss: 288.661
iter:2000 | loss: 193.561
iter:2100 | loss: 151.147
iter:2200 | loss: 185.129
iter:2300 | loss: 174.896
iter:2400 | loss: 133.615
iter:2500 | loss: 163.707
iter:2600 | loss: 164.399
iter:2700 | loss: 104.809
iter:2800 | loss: 167.818
iter:2900 | loss: 157.640
iter:3000 | loss: 176.894
iter:3100 | loss: 209.997
iter:3200 | loss: 170.114
iter:3300 | loss: 143.053
iter:3400 | loss: 143.366
iter:3500 | loss: 153.173
iter:3600 | loss: 168.382
iter:3700 | loss: 229.099
iter:3800 | loss: 239.052
iter:3900 | loss: 144.765
| end of epoch   2 | time: 858.07s | train_loss 177.2429 | valid rse 0.0562 | valid rae 0.0402 | valid corr  0.9269
iter:  0 | loss: 144.638
iter:100 | loss: 159.957
iter:200 | loss: 153.279
iter:300 | loss: 161.170
iter:400 | loss: 153.742
iter:500 | loss: 143.436
iter:600 | loss: 108.558
iter:700 | loss: 190.039
iter:800 | loss: 111.691
iter:900 | loss: 204.619
iter:1000 | loss: 129.277
iter:1100 | loss: 154.786
iter:1200 | loss: 145.826
iter:1300 | loss: 132.952
iter:1400 | loss: 123.573
iter:1500 | loss: 233.790
iter:1600 | loss: 150.936
iter:1700 | loss: 163.356
iter:1800 | loss: 143.703
iter:1900 | loss: 152.737
iter:2000 | loss: 174.982
iter:2100 | loss: 137.813
iter:2200 | loss: 196.701
iter:2300 | loss: 161.721
iter:2400 | loss: 126.537
iter:2500 | loss: 154.956
iter:2600 | loss: 192.370
iter:2700 | loss: 128.794
iter:2800 | loss: 145.532
iter:2900 | loss: 118.919
iter:3000 | loss: 189.971
iter:3100 | loss: 132.386
iter:3200 | loss: 128.573
iter:3300 | loss: 204.428
iter:3400 | loss: 126.835
iter:3500 | loss: 137.173
iter:3600 | loss: 195.544
iter:3700 | loss: 175.976
iter:3800 | loss: 154.951
iter:3900 | loss: 121.883
| end of epoch   3 | time: 857.65s | train_loss 168.6902 | valid rse 0.0561 | valid rae 0.0408 | valid corr  0.9282
iter:  0 | loss: 158.200
iter:100 | loss: 160.092
iter:200 | loss: 225.410
iter:300 | loss: 142.972
iter:400 | loss: 137.475
iter:500 | loss: 142.364
iter:600 | loss: 154.943
iter:700 | loss: 126.680
iter:800 | loss: 205.142
iter:900 | loss: 178.185
iter:1000 | loss: 197.615
iter:1100 | loss: 143.812
iter:1200 | loss: 172.572
iter:1300 | loss: 189.202
iter:1400 | loss: 285.880
iter:1500 | loss: 129.104
iter:1600 | loss: 188.685
iter:1700 | loss: 186.988
iter:1800 | loss: 142.012
iter:1900 | loss: 132.327
iter:2000 | loss: 112.947
iter:2100 | loss: 120.190
iter:2200 | loss: 112.722
iter:2300 | loss: 127.616
iter:2400 | loss: 165.419
iter:2500 | loss: 110.828
iter:2600 | loss: 142.627
iter:2700 | loss: 141.000
iter:2800 | loss: 138.887
iter:2900 | loss: 168.210
iter:3000 | loss: 112.497
iter:3100 | loss: 179.934
iter:3200 | loss: 140.967
iter:3300 | loss: 115.740
iter:3400 | loss: 166.226
iter:3500 | loss: 125.008
iter:3600 | loss: 110.299
iter:3700 | loss: 146.272
iter:3800 | loss: 134.403
iter:3900 | loss: 155.586
| end of epoch   4 | time: 856.67s | train_loss 163.7764 | valid rse 0.0553 | valid rae 0.0396 | valid corr  0.9300
iter:  0 | loss: 140.262
iter:100 | loss: 143.808
iter:200 | loss: 259.594
iter:300 | loss: 141.492
iter:400 | loss: 243.605
iter:500 | loss: 153.903
iter:600 | loss: 122.033
iter:700 | loss: 146.738
iter:800 | loss: 193.525
iter:900 | loss: 155.824
iter:1000 | loss: 207.129
iter:1100 | loss: 134.726
iter:1200 | loss: 127.593
iter:1300 | loss: 142.268
iter:1400 | loss: 218.334
iter:1500 | loss: 161.213
iter:1600 | loss: 133.592
iter:1700 | loss: 103.622
iter:1800 | loss: 157.511
iter:1900 | loss: 151.328
iter:2000 | loss: 155.461
iter:2100 | loss: 135.987
iter:2200 | loss: 141.420
iter:2300 | loss: 140.709
iter:2400 | loss: 124.107
iter:2500 | loss: 159.721
iter:2600 | loss: 205.677
iter:2700 | loss: 169.418
iter:2800 | loss: 142.190
iter:2900 | loss: 202.896
iter:3000 | loss: 215.403
iter:3100 | loss: 152.206
iter:3200 | loss: 99.883
iter:3300 | loss: 125.363
iter:3400 | loss: 162.017
iter:3500 | loss: 139.078
iter:3600 | loss: 190.666
iter:3700 | loss: 293.367
iter:3800 | loss: 127.364
iter:3900 | loss: 146.703
| end of epoch   5 | time: 855.87s | train_loss 161.1316 | valid rse 0.0546 | valid rae 0.0381 | valid corr  0.9312
test rse 0.0791 | test rae 0.0444 | test corr 0.9392
iter:  0 | loss: 136.441
iter:100 | loss: 149.222
iter:200 | loss: 249.914
iter:300 | loss: 148.445
iter:400 | loss: 134.980
iter:500 | loss: 178.314
iter:600 | loss: 241.344
iter:700 | loss: 152.558
iter:800 | loss: 184.197
iter:900 | loss: 115.598
iter:1000 | loss: 153.135
iter:1100 | loss: 142.929
iter:1200 | loss: 138.735
iter:1300 | loss: 174.274
iter:1400 | loss: 225.775
iter:1500 | loss: 109.979
iter:1600 | loss: 141.845
iter:1700 | loss: 137.077
iter:1800 | loss: 149.260
iter:1900 | loss: 135.640
iter:2000 | loss: 102.113
iter:2100 | loss: 118.271
iter:2200 | loss: 119.584
iter:2300 | loss: 133.458
iter:2400 | loss: 179.022
iter:2500 | loss: 99.832
iter:2600 | loss: 93.231
iter:2700 | loss: 206.770
iter:2800 | loss: 111.911
iter:2900 | loss: 177.777
iter:3000 | loss: 98.182
iter:3100 | loss: 125.797
iter:3200 | loss: 203.714
iter:3300 | loss: 221.192
iter:3400 | loss: 159.796
iter:3500 | loss: 233.292
iter:3600 | loss: 110.800
iter:3700 | loss: 151.522
iter:3800 | loss: 133.993
iter:3900 | loss: 179.606
| end of epoch   6 | time: 856.98s | train_loss 159.0297 | valid rse 0.0523 | valid rae 0.0372 | valid corr  0.9335
iter:  0 | loss: 136.114
iter:100 | loss: 183.428
iter:200 | loss: 130.886
iter:300 | loss: 187.329
iter:400 | loss: 174.338
iter:500 | loss: 135.789
iter:600 | loss: 288.141
iter:700 | loss: 117.551
iter:800 | loss: 120.374
iter:900 | loss: 155.264
iter:1000 | loss: 97.161
iter:1100 | loss: 153.307
iter:1200 | loss: 165.263
iter:1300 | loss: 199.508
iter:1400 | loss: 182.945
iter:1500 | loss: 162.479
iter:1600 | loss: 114.527
iter:1700 | loss: 120.817
iter:1800 | loss: 141.542
iter:1900 | loss: 126.440
iter:2000 | loss: 131.676
iter:2100 | loss: 101.348
iter:2200 | loss: 146.193
iter:2300 | loss: 133.857
iter:2400 | loss: 154.534
iter:2500 | loss: 132.424
iter:2600 | loss: 114.501
iter:2700 | loss: 96.338
iter:2800 | loss: 104.763
iter:2900 | loss: 143.091
iter:3000 | loss: 183.436
iter:3100 | loss: 116.670
iter:3200 | loss: 102.975
iter:3300 | loss: 197.141
iter:3400 | loss: 144.245
iter:3500 | loss: 138.309
iter:3600 | loss: 152.834
iter:3700 | loss: 114.705
iter:3800 | loss: 146.966
iter:3900 | loss: 123.074
| end of epoch   7 | time: 856.57s | train_loss 157.1496 | valid rse 0.0525 | valid rae 0.0380 | valid corr  0.9342
iter:  0 | loss: 133.141
iter:100 | loss: 224.377
iter:200 | loss: 137.971
iter:300 | loss: 118.443
iter:400 | loss: 184.904
iter:500 | loss: 179.194
iter:600 | loss: 206.417
iter:700 | loss: 166.679
iter:800 | loss: 136.311
iter:900 | loss: 165.975
iter:1000 | loss: 156.487
iter:1100 | loss: 155.642
iter:1200 | loss: 111.782
iter:1300 | loss: 150.715
iter:1400 | loss: 120.215
iter:1500 | loss: 89.093
iter:1600 | loss: 217.082
iter:1700 | loss: 125.022
iter:1800 | loss: 134.183
iter:1900 | loss: 206.171
iter:2000 | loss: 135.884
iter:2100 | loss: 174.737
iter:2200 | loss: 173.790
iter:2300 | loss: 129.565
iter:2400 | loss: 160.106
iter:2500 | loss: 189.095
iter:2600 | loss: 138.862
iter:2700 | loss: 244.844
iter:2800 | loss: 163.529
iter:2900 | loss: 169.162
iter:3000 | loss: 151.933
iter:3100 | loss: 126.741
iter:3200 | loss: 118.212
iter:3300 | loss: 244.605
iter:3400 | loss: 114.704
iter:3500 | loss: 114.142
iter:3600 | loss: 170.060
iter:3700 | loss: 100.613
iter:3800 | loss: 176.432
iter:3900 | loss: 160.570
| end of epoch   8 | time: 856.57s | train_loss 156.1459 | valid rse 0.0516 | valid rae 0.0365 | valid corr  0.9343
iter:  0 | loss: 106.576
iter:100 | loss: 157.693
iter:200 | loss: 96.879
iter:300 | loss: 194.612
iter:400 | loss: 156.572
iter:500 | loss: 212.801
iter:600 | loss: 131.942
iter:700 | loss: 177.638
iter:800 | loss: 163.258
iter:900 | loss: 128.811
iter:1000 | loss: 145.197
iter:1100 | loss: 105.387
iter:1200 | loss: 184.069
iter:1300 | loss: 144.235
iter:1400 | loss: 127.797
iter:1500 | loss: 133.308
iter:1600 | loss: 121.458
iter:1700 | loss: 190.040
iter:1800 | loss: 178.669
iter:1900 | loss: 157.852
iter:2000 | loss: 151.194
iter:2100 | loss: 168.089
iter:2200 | loss: 156.530
iter:2300 | loss: 123.115
iter:2400 | loss: 148.842
iter:2500 | loss: 156.638
iter:2600 | loss: 132.061
iter:2700 | loss: 137.910
iter:2800 | loss: 157.136
iter:2900 | loss: 145.119
iter:3000 | loss: 161.843
iter:3100 | loss: 136.048
iter:3200 | loss: 154.025
iter:3300 | loss: 169.518
iter:3400 | loss: 119.752
iter:3500 | loss: 151.872
iter:3600 | loss: 179.530
iter:3700 | loss: 132.473
iter:3800 | loss: 139.970
iter:3900 | loss: 106.391
| end of epoch   9 | time: 857.33s | train_loss 154.5524 | valid rse 0.0521 | valid rae 0.0370 | valid corr  0.9347
iter:  0 | loss: 128.455
iter:100 | loss: 138.242
iter:200 | loss: 229.734
iter:300 | loss: 115.191
iter:400 | loss: 105.829
iter:500 | loss: 147.765
iter:600 | loss: 101.245
iter:700 | loss: 168.164
iter:800 | loss: 117.817
iter:900 | loss: 239.714
iter:1000 | loss: 163.055
iter:1100 | loss: 164.798
iter:1200 | loss: 138.673
iter:1300 | loss: 187.224
iter:1400 | loss: 99.438
iter:1500 | loss: 222.626
iter:1600 | loss: 139.116
iter:1700 | loss: 318.458
iter:1800 | loss: 206.890
iter:1900 | loss: 114.972
iter:2000 | loss: 237.741
iter:2100 | loss: 143.937
iter:2200 | loss: 146.987
iter:2300 | loss: 142.431
iter:2400 | loss: 113.794
iter:2500 | loss: 207.184
iter:2600 | loss: 123.223
iter:2700 | loss: 183.559
iter:2800 | loss: 99.249
iter:2900 | loss: 210.272
iter:3000 | loss: 241.025
iter:3100 | loss: 107.733
iter:3200 | loss: 207.435
iter:3300 | loss: 123.573
iter:3400 | loss: 164.301
iter:3500 | loss: 155.246
iter:3600 | loss: 126.966
iter:3700 | loss: 178.117
iter:3800 | loss: 211.757
iter:3900 | loss: 141.100
| end of epoch  10 | time: 858.24s | train_loss 154.0264 | valid rse 0.0531 | valid rae 0.0371 | valid corr  0.9353
test rse 0.0776 | test rae 0.0436 | test corr 0.9426
iter:  0 | loss: 146.531
iter:100 | loss: 187.797
iter:200 | loss: 168.927
iter:300 | loss: 134.716
iter:400 | loss: 101.226
iter:500 | loss: 147.128
iter:600 | loss: 128.016
iter:700 | loss: 187.101
iter:800 | loss: 208.136
iter:900 | loss: 253.052
iter:1000 | loss: 247.114
iter:1100 | loss: 177.787
iter:1200 | loss: 133.458
iter:1300 | loss: 183.407
iter:1400 | loss: 139.032
iter:1500 | loss: 250.677
iter:1600 | loss: 194.553
iter:1700 | loss: 173.490
iter:1800 | loss: 104.206
iter:1900 | loss: 155.157
iter:2000 | loss: 132.188
iter:2100 | loss: 141.248
iter:2200 | loss: 114.361
iter:2300 | loss: 123.926
iter:2400 | loss: 170.555
iter:2500 | loss: 124.415
iter:2600 | loss: 170.113
iter:2700 | loss: 117.292
iter:2800 | loss: 254.087
iter:2900 | loss: 169.464
iter:3000 | loss: 143.865
iter:3100 | loss: 121.553
iter:3200 | loss: 240.805
iter:3300 | loss: 122.920
iter:3400 | loss: 211.647
iter:3500 | loss: 147.077
iter:3600 | loss: 163.959
iter:3700 | loss: 174.818
iter:3800 | loss: 456.625
iter:3900 | loss: 194.899
| end of epoch  11 | time: 858.18s | train_loss 153.3792 | valid rse 0.0517 | valid rae 0.0371 | valid corr  0.9357
iter:  0 | loss: 192.597
iter:100 | loss: 125.651
iter:200 | loss: 231.386
iter:300 | loss: 141.857
iter:400 | loss: 138.667
iter:500 | loss: 160.029
iter:600 | loss: 148.459
iter:700 | loss: 110.939
iter:800 | loss: 146.378
iter:900 | loss: 160.857
iter:1000 | loss: 164.204
iter:1100 | loss: 138.157
iter:1200 | loss: 141.716
iter:1300 | loss: 116.890
iter:1400 | loss: 158.626
iter:1500 | loss: 247.215
iter:1600 | loss: 137.913
iter:1700 | loss: 167.100
iter:1800 | loss: 186.421
iter:1900 | loss: 164.071
iter:2000 | loss: 162.609
iter:2100 | loss: 121.395
iter:2200 | loss: 144.406
iter:2300 | loss: 238.900
iter:2400 | loss: 120.026
iter:2500 | loss: 297.560
iter:2600 | loss: 210.769
iter:2700 | loss: 117.831
iter:2800 | loss: 117.805
iter:2900 | loss: 99.498
iter:3000 | loss: 145.200
iter:3100 | loss: 149.634
iter:3200 | loss: 137.472
iter:3300 | loss: 139.960
iter:3400 | loss: 239.805
iter:3500 | loss: 147.097
iter:3600 | loss: 118.778
iter:3700 | loss: 175.605
iter:3800 | loss: 281.196
iter:3900 | loss: 123.218
| end of epoch  12 | time: 857.05s | train_loss 152.0295 | valid rse 0.0512 | valid rae 0.0367 | valid corr  0.9365
iter:  0 | loss: 205.950
iter:100 | loss: 159.305
iter:200 | loss: 144.360
iter:300 | loss: 132.079
iter:400 | loss: 208.724
iter:500 | loss: 134.797
iter:600 | loss: 209.213
iter:700 | loss: 146.548
iter:800 | loss: 157.355
iter:900 | loss: 116.997
iter:1000 | loss: 150.090
iter:1100 | loss: 108.696
iter:1200 | loss: 205.084
iter:1300 | loss: 206.481
iter:1400 | loss: 164.251
iter:1500 | loss: 126.767
iter:1600 | loss: 166.497
iter:1700 | loss: 177.451
iter:1800 | loss: 79.913
iter:1900 | loss: 124.681
iter:2000 | loss: 153.371
iter:2100 | loss: 189.612
iter:2200 | loss: 130.165
iter:2300 | loss: 143.872
iter:2400 | loss: 200.347
iter:2500 | loss: 171.848
iter:2600 | loss: 114.489
iter:2700 | loss: 105.555
iter:2800 | loss: 102.090
iter:2900 | loss: 128.301
iter:3000 | loss: 112.112
iter:3100 | loss: 199.823
iter:3200 | loss: 113.382
iter:3300 | loss: 117.087
iter:3400 | loss: 169.513
iter:3500 | loss: 126.232
iter:3600 | loss: 111.332
iter:3700 | loss: 98.818
iter:3800 | loss: 136.519
iter:3900 | loss: 165.357
| end of epoch  13 | time: 858.16s | train_loss 151.2716 | valid rse 0.0525 | valid rae 0.0370 | valid corr  0.9377
iter:  0 | loss: 134.909
iter:100 | loss: 203.099
iter:200 | loss: 132.873
iter:300 | loss: 127.266
iter:400 | loss: 149.247
iter:500 | loss: 174.469
iter:600 | loss: 103.385
iter:700 | loss: 183.818
iter:800 | loss: 153.499
iter:900 | loss: 164.716
iter:1000 | loss: 138.410
iter:1100 | loss: 109.291
iter:1200 | loss: 107.468
iter:1300 | loss: 119.503
iter:1400 | loss: 136.310
iter:1500 | loss: 111.844
iter:1600 | loss: 116.147
iter:1700 | loss: 99.671
iter:1800 | loss: 214.936
iter:1900 | loss: 149.110
iter:2000 | loss: 186.570
iter:2100 | loss: 161.181
iter:2200 | loss: 125.467
iter:2300 | loss: 112.437
iter:2400 | loss: 140.629
iter:2500 | loss: 117.131
iter:2600 | loss: 187.141
iter:2700 | loss: 118.822
iter:2800 | loss: 117.547
iter:2900 | loss: 266.985
iter:3000 | loss: 120.729
iter:3100 | loss: 185.636
iter:3200 | loss: 148.176
iter:3300 | loss: 116.853
iter:3400 | loss: 142.211
iter:3500 | loss: 135.038
iter:3600 | loss: 182.780
iter:3700 | loss: 156.793
iter:3800 | loss: 147.800
iter:3900 | loss: 211.195
| end of epoch  14 | time: 857.21s | train_loss 150.3312 | valid rse 0.0510 | valid rae 0.0363 | valid corr  0.9379
iter:  0 | loss: 103.277
iter:100 | loss: 171.985
iter:200 | loss: 128.944
iter:300 | loss: 108.982
iter:400 | loss: 153.756
iter:500 | loss: 130.217
iter:600 | loss: 108.327
iter:700 | loss: 150.883
iter:800 | loss: 93.738
iter:900 | loss: 177.567
iter:1000 | loss: 160.254
iter:1100 | loss: 134.034
iter:1200 | loss: 133.778
iter:1300 | loss: 132.857
iter:1400 | loss: 146.028
iter:1500 | loss: 166.278
iter:1600 | loss: 104.960
iter:1700 | loss: 175.251
iter:1800 | loss: 169.676
iter:1900 | loss: 163.543
iter:2000 | loss: 187.587
iter:2100 | loss: 106.768
iter:2200 | loss: 181.652
iter:2300 | loss: 175.468
iter:2400 | loss: 105.937
iter:2500 | loss: 90.729
iter:2600 | loss: 187.777
iter:2700 | loss: 132.278
iter:2800 | loss: 111.311
iter:2900 | loss: 179.732
iter:3000 | loss: 169.378
iter:3100 | loss: 108.096
iter:3200 | loss: 155.983
iter:3300 | loss: 155.136
iter:3400 | loss: 103.917
iter:3500 | loss: 131.602
iter:3600 | loss: 124.736
iter:3700 | loss: 143.915
iter:3800 | loss: 158.676
iter:3900 | loss: 106.241
| end of epoch  15 | time: 856.64s | train_loss 149.0958 | valid rse 0.0515 | valid rae 0.0372 | valid corr  0.9382
test rse 0.0769 | test rae 0.0441 | test corr 0.9444
iter:  0 | loss: 104.778
iter:100 | loss: 207.085
iter:200 | loss: 177.691
iter:300 | loss: 182.900
iter:400 | loss: 277.182
iter:500 | loss: 334.232
iter:600 | loss: 208.683
iter:700 | loss: 170.319
iter:800 | loss: 154.817
iter:900 | loss: 127.004
iter:1000 | loss: 253.399
iter:1100 | loss: 186.559
iter:1200 | loss: 144.991
iter:1300 | loss: 105.744
iter:1400 | loss: 167.879
iter:1500 | loss: 116.357
iter:1600 | loss: 137.227
iter:1700 | loss: 232.850
iter:1800 | loss: 122.346
iter:1900 | loss: 99.952
iter:2000 | loss: 198.613
iter:2100 | loss: 164.901
iter:2200 | loss: 124.386
iter:2300 | loss: 152.385
iter:2400 | loss: 155.977
iter:2500 | loss: 139.507
iter:2600 | loss: 114.231
iter:2700 | loss: 118.708
iter:2800 | loss: 166.648
iter:2900 | loss: 125.087
iter:3000 | loss: 112.444
iter:3100 | loss: 264.043
iter:3200 | loss: 143.995
iter:3300 | loss: 198.982
iter:3400 | loss: 162.241
iter:3500 | loss: 164.228
iter:3600 | loss: 97.209
iter:3700 | loss: 180.249
iter:3800 | loss: 112.348
iter:3900 | loss: 99.913
| end of epoch  16 | time: 855.85s | train_loss 149.0117 | valid rse 0.0502 | valid rae 0.0353 | valid corr  0.9383
iter:  0 | loss: 133.492
iter:100 | loss: 172.710
iter:200 | loss: 134.840
iter:300 | loss: 113.499
iter:400 | loss: 136.380
iter:500 | loss: 126.656
iter:600 | loss: 168.759
iter:700 | loss: 111.079
iter:800 | loss: 137.614
iter:900 | loss: 137.624
iter:1000 | loss: 102.465
iter:1100 | loss: 142.093
iter:1200 | loss: 225.590
iter:1300 | loss: 102.220
iter:1400 | loss: 239.229
iter:1500 | loss: 125.169
iter:1600 | loss: 111.734
iter:1700 | loss: 247.958
iter:1800 | loss: 136.467
iter:1900 | loss: 149.864
iter:2000 | loss: 140.042
iter:2100 | loss: 163.227
iter:2200 | loss: 198.722
iter:2300 | loss: 183.927
iter:2400 | loss: 132.348
iter:2500 | loss: 133.255
iter:2600 | loss: 300.282
iter:2700 | loss: 142.932
iter:2800 | loss: 150.540
iter:2900 | loss: 150.676
iter:3000 | loss: 134.199
iter:3100 | loss: 132.511
iter:3200 | loss: 158.658
iter:3300 | loss: 178.918
iter:3400 | loss: 125.561
iter:3500 | loss: 132.611
iter:3600 | loss: 181.772
iter:3700 | loss: 235.868
iter:3800 | loss: 196.644
iter:3900 | loss: 115.346
| end of epoch  17 | time: 858.01s | train_loss 148.2488 | valid rse 0.0503 | valid rae 0.0351 | valid corr  0.9380
iter:  0 | loss: 391.412
iter:100 | loss: 145.657
iter:200 | loss: 155.967
iter:300 | loss: 177.482
iter:400 | loss: 92.563
iter:500 | loss: 95.132
iter:600 | loss: 92.746
iter:700 | loss: 132.842
iter:800 | loss: 139.476
iter:900 | loss: 146.555
iter:1000 | loss: 114.598
iter:1100 | loss: 143.841
iter:1200 | loss: 162.457
iter:1300 | loss: 123.569
iter:1400 | loss: 110.954
iter:1500 | loss: 274.953
iter:1600 | loss: 107.117
iter:1700 | loss: 152.601
iter:1800 | loss: 160.674
iter:1900 | loss: 238.058
iter:2000 | loss: 88.660
iter:2100 | loss: 137.807
iter:2200 | loss: 157.921
iter:2300 | loss: 80.056
iter:2400 | loss: 130.111
iter:2500 | loss: 92.370
iter:2600 | loss: 246.409
iter:2700 | loss: 138.031
iter:2800 | loss: 126.093
iter:2900 | loss: 209.577
iter:3000 | loss: 139.994
iter:3100 | loss: 223.195
iter:3200 | loss: 105.654
iter:3300 | loss: 146.942
iter:3400 | loss: 106.634
iter:3500 | loss: 184.395
iter:3600 | loss: 159.474
iter:3700 | loss: 125.531
iter:3800 | loss: 125.696
iter:3900 | loss: 215.835
| end of epoch  18 | time: 857.34s | train_loss 147.7983 | valid rse 0.0536 | valid rae 0.0379 | valid corr  0.9392
iter:  0 | loss: 102.726
iter:100 | loss: 174.003
iter:200 | loss: 141.259
iter:300 | loss: 174.687
iter:400 | loss: 137.383
iter:500 | loss: 213.125
iter:600 | loss: 175.301
iter:700 | loss: 167.247
iter:800 | loss: 120.793
iter:900 | loss: 362.435
iter:1000 | loss: 127.348
iter:1100 | loss: 150.701
iter:1200 | loss: 116.586
iter:1300 | loss: 169.050
iter:1400 | loss: 237.107
iter:1500 | loss: 134.219
iter:1600 | loss: 117.360
iter:1700 | loss: 197.359
iter:1800 | loss: 97.709
iter:1900 | loss: 126.829
iter:2000 | loss: 188.977
iter:2100 | loss: 134.081
iter:2200 | loss: 131.644
iter:2300 | loss: 131.470
iter:2400 | loss: 116.706
iter:2500 | loss: 127.546
iter:2600 | loss: 96.339
iter:2700 | loss: 172.917
iter:2800 | loss: 139.146
iter:2900 | loss: 133.091
iter:3000 | loss: 134.934
iter:3100 | loss: 194.125
iter:3200 | loss: 158.633
iter:3300 | loss: 209.409
iter:3400 | loss: 133.569
iter:3500 | loss: 129.191
iter:3600 | loss: 129.625
iter:3700 | loss: 185.052
iter:3800 | loss: 156.650
iter:3900 | loss: 107.692
| end of epoch  19 | time: 856.81s | train_loss 147.4369 | valid rse 0.0526 | valid rae 0.0371 | valid corr  0.9395
iter:  0 | loss: 123.193
iter:100 | loss: 137.780
iter:200 | loss: 216.401
iter:300 | loss: 122.983
iter:400 | loss: 147.521
iter:500 | loss: 145.348
iter:600 | loss: 114.281
iter:700 | loss: 161.704
iter:800 | loss: 179.307
iter:900 | loss: 105.391
iter:1000 | loss: 148.910
iter:1100 | loss: 163.668
iter:1200 | loss: 127.798
iter:1300 | loss: 99.758
iter:1400 | loss: 166.946
iter:1500 | loss: 154.362
iter:1600 | loss: 132.925
iter:1700 | loss: 175.879
iter:1800 | loss: 117.643
iter:1900 | loss: 129.154
iter:2000 | loss: 83.807
iter:2100 | loss: 106.989
iter:2200 | loss: 195.793
iter:2300 | loss: 262.146
iter:2400 | loss: 120.411
iter:2500 | loss: 160.705
iter:2600 | loss: 178.406
iter:2700 | loss: 137.078
iter:2800 | loss: 315.110
iter:2900 | loss: 145.977
iter:3000 | loss: 109.282
iter:3100 | loss: 133.340
iter:3200 | loss: 157.428
iter:3300 | loss: 168.252
iter:3400 | loss: 116.635
iter:3500 | loss: 157.768
iter:3600 | loss: 131.660
iter:3700 | loss: 130.982
iter:3800 | loss: 130.183
iter:3900 | loss: 184.542
| end of epoch  20 | time: 860.19s | train_loss 146.7833 | valid rse 0.0509 | valid rae 0.0360 | valid corr  0.9397
test rse 0.0760 | test rae 0.0426 | test corr 0.9463
iter:  0 | loss: 166.438
iter:100 | loss: 143.326
iter:200 | loss: 158.180
iter:300 | loss: 97.177
iter:400 | loss: 110.613
iter:500 | loss: 166.448
iter:600 | loss: 116.323
iter:700 | loss: 142.176
iter:800 | loss: 132.732
iter:900 | loss: 170.174
iter:1000 | loss: 130.736
iter:1100 | loss: 181.533
iter:1200 | loss: 168.501
iter:1300 | loss: 131.043
iter:1400 | loss: 114.619
iter:1500 | loss: 173.904
iter:1600 | loss: 136.049
iter:1700 | loss: 109.702
iter:1800 | loss: 157.649
iter:1900 | loss: 103.587
iter:2000 | loss: 152.034
iter:2100 | loss: 163.727
iter:2200 | loss: 207.980
iter:2300 | loss: 105.893
iter:2400 | loss: 139.246
iter:2500 | loss: 162.833
iter:2600 | loss: 154.658
iter:2700 | loss: 123.669
iter:2800 | loss: 223.841
iter:2900 | loss: 167.302
iter:3000 | loss: 131.014
iter:3100 | loss: 99.833
iter:3200 | loss: 247.438
iter:3300 | loss: 136.738
iter:3400 | loss: 152.865
iter:3500 | loss: 188.222
iter:3600 | loss: 112.018
iter:3700 | loss: 146.148
iter:3800 | loss: 139.414
iter:3900 | loss: 189.153
| end of epoch  21 | time: 856.30s | train_loss 146.3908 | valid rse 0.0511 | valid rae 0.0357 | valid corr  0.9404
iter:  0 | loss: 142.884
iter:100 | loss: 92.989
iter:200 | loss: 126.329
iter:300 | loss: 140.476
iter:400 | loss: 101.756
iter:500 | loss: 190.154
iter:600 | loss: 228.551
iter:700 | loss: 141.305
iter:800 | loss: 125.672
iter:900 | loss: 119.694
iter:1000 | loss: 237.024
iter:1100 | loss: 87.238
iter:1200 | loss: 105.497
iter:1300 | loss: 203.480
iter:1400 | loss: 115.659
iter:1500 | loss: 114.801
iter:1600 | loss: 129.904
iter:1700 | loss: 156.937
iter:1800 | loss: 113.661
iter:1900 | loss: 120.724
iter:2000 | loss: 237.158
iter:2100 | loss: 123.058
iter:2200 | loss: 188.233
iter:2300 | loss: 114.684
iter:2400 | loss: 150.172
iter:2500 | loss: 129.403
iter:2600 | loss: 105.786
iter:2700 | loss: 181.665
iter:2800 | loss: 171.951
iter:2900 | loss: 219.053
iter:3000 | loss: 152.592
iter:3100 | loss: 127.849
iter:3200 | loss: 136.672
iter:3300 | loss: 159.174
iter:3400 | loss: 146.434
iter:3500 | loss: 123.842
iter:3600 | loss: 227.925
iter:3700 | loss: 244.223
iter:3800 | loss: 129.087
iter:3900 | loss: 147.534
| end of epoch  22 | time: 856.71s | train_loss 145.8369 | valid rse 0.0513 | valid rae 0.0352 | valid corr  0.9402
iter:  0 | loss: 105.559
iter:100 | loss: 161.231
iter:200 | loss: 134.013
iter:300 | loss: 125.234
iter:400 | loss: 148.667
iter:500 | loss: 169.530
iter:600 | loss: 99.838
iter:700 | loss: 121.235
iter:800 | loss: 89.275
iter:900 | loss: 148.044
iter:1000 | loss: 129.681
iter:1100 | loss: 131.800
iter:1200 | loss: 126.699
iter:1300 | loss: 110.818
iter:1400 | loss: 108.878
iter:1500 | loss: 117.146
iter:1600 | loss: 111.277
iter:1700 | loss: 150.331
iter:1800 | loss: 156.790
iter:1900 | loss: 122.434
iter:2000 | loss: 142.891
iter:2100 | loss: 121.846
iter:2200 | loss: 136.917
iter:2300 | loss: 95.356
iter:2400 | loss: 149.275
iter:2500 | loss: 128.275
iter:2600 | loss: 160.082
iter:2700 | loss: 112.335
iter:2800 | loss: 151.110
iter:2900 | loss: 164.045
iter:3000 | loss: 115.945
iter:3100 | loss: 148.092
iter:3200 | loss: 173.959
iter:3300 | loss: 159.298
iter:3400 | loss: 130.333
iter:3500 | loss: 125.184
iter:3600 | loss: 140.768
iter:3700 | loss: 141.149
iter:3800 | loss: 99.989
iter:3900 | loss: 183.170
| end of epoch  23 | time: 857.20s | train_loss 145.7071 | valid rse 0.0517 | valid rae 0.0378 | valid corr  0.9399
iter:  0 | loss: 182.504
iter:100 | loss: 143.257
iter:200 | loss: 92.613
iter:300 | loss: 214.464
iter:400 | loss: 192.371
iter:500 | loss: 143.252
iter:600 | loss: 95.589
iter:700 | loss: 224.889
iter:800 | loss: 225.242
iter:900 | loss: 156.441
iter:1000 | loss: 133.609
iter:1100 | loss: 113.744
iter:1200 | loss: 88.170
iter:1300 | loss: 151.408
iter:1400 | loss: 131.853
iter:1500 | loss: 162.637
iter:1600 | loss: 182.780
iter:1700 | loss: 153.527
iter:1800 | loss: 107.819
iter:1900 | loss: 141.648
iter:2000 | loss: 140.850
iter:2100 | loss: 166.504
iter:2200 | loss: 193.727
iter:2300 | loss: 125.495
iter:2400 | loss: 88.941
iter:2500 | loss: 130.751
iter:2600 | loss: 145.325
iter:2700 | loss: 112.991
iter:2800 | loss: 122.064
iter:2900 | loss: 127.164
iter:3000 | loss: 224.391
iter:3100 | loss: 113.129
iter:3200 | loss: 136.059
iter:3300 | loss: 132.421
iter:3400 | loss: 135.307
iter:3500 | loss: 158.739
iter:3600 | loss: 163.111
iter:3700 | loss: 110.191
iter:3800 | loss: 110.362
iter:3900 | loss: 145.175
| end of epoch  24 | time: 857.85s | train_loss 145.0823 | valid rse 0.0508 | valid rae 0.0352 | valid corr  0.9408
iter:  0 | loss: 118.255
iter:100 | loss: 138.065
iter:200 | loss: 189.093
iter:300 | loss: 153.180
iter:400 | loss: 167.500
iter:500 | loss: 159.371
iter:600 | loss: 155.196
iter:700 | loss: 118.126
iter:800 | loss: 154.634
iter:900 | loss: 135.244
iter:1000 | loss: 114.222
iter:1100 | loss: 140.580
iter:1200 | loss: 99.739
iter:1300 | loss: 191.435
iter:1400 | loss: 137.866
iter:1500 | loss: 128.633
iter:1600 | loss: 108.170
iter:1700 | loss: 206.268
iter:1800 | loss: 123.989
iter:1900 | loss: 121.333
iter:2000 | loss: 102.494
iter:2100 | loss: 114.520
iter:2200 | loss: 161.193
iter:2300 | loss: 110.098
iter:2400 | loss: 148.090
iter:2500 | loss: 134.360
iter:2600 | loss: 112.085
iter:2700 | loss: 177.769
iter:2800 | loss: 126.197
iter:2900 | loss: 195.604
iter:3000 | loss: 180.969
iter:3100 | loss: 166.287
iter:3200 | loss: 142.711
iter:3300 | loss: 122.163
iter:3400 | loss: 118.640
iter:3500 | loss: 98.829
iter:3600 | loss: 136.532
iter:3700 | loss: 265.861
iter:3800 | loss: 150.940
iter:3900 | loss: 145.911
| end of epoch  25 | time: 858.40s | train_loss 144.9337 | valid rse 0.0521 | valid rae 0.0367 | valid corr  0.9407
test rse 0.0757 | test rae 0.0429 | test corr 0.9472
iter:  0 | loss: 122.228
iter:100 | loss: 140.825
iter:200 | loss: 161.024
iter:300 | loss: 121.788
iter:400 | loss: 91.950
iter:500 | loss: 110.195
iter:600 | loss: 166.020
iter:700 | loss: 96.509
iter:800 | loss: 181.288
iter:900 | loss: 140.947
iter:1000 | loss: 132.905
iter:1100 | loss: 126.092
iter:1200 | loss: 197.543
iter:1300 | loss: 87.841
iter:1400 | loss: 108.107
iter:1500 | loss: 128.747
iter:1600 | loss: 154.325
iter:1700 | loss: 163.819
iter:1800 | loss: 113.037
iter:1900 | loss: 101.441
iter:2000 | loss: 171.780
iter:2100 | loss: 127.618
iter:2200 | loss: 102.453
iter:2300 | loss: 128.422
iter:2400 | loss: 137.538
iter:2500 | loss: 118.401
iter:2600 | loss: 176.768
iter:2700 | loss: 168.976
iter:2800 | loss: 117.809
iter:2900 | loss: 160.813
iter:3000 | loss: 117.759
iter:3100 | loss: 150.018
iter:3200 | loss: 100.187
iter:3300 | loss: 127.726
iter:3400 | loss: 123.666
iter:3500 | loss: 183.524
iter:3600 | loss: 136.290
iter:3700 | loss: 145.238
iter:3800 | loss: 173.259
iter:3900 | loss: 102.499
| end of epoch  26 | time: 857.63s | train_loss 144.0302 | valid rse 0.0503 | valid rae 0.0356 | valid corr  0.9414
iter:  0 | loss: 96.454
iter:100 | loss: 160.276
iter:200 | loss: 185.602
iter:300 | loss: 179.137
iter:400 | loss: 108.484
iter:500 | loss: 116.032
iter:600 | loss: 144.217
iter:700 | loss: 175.205
iter:800 | loss: 147.260
iter:900 | loss: 121.793
iter:1000 | loss: 139.913
iter:1100 | loss: 119.349
iter:1200 | loss: 158.232
iter:1300 | loss: 175.536
iter:1400 | loss: 147.824
iter:1500 | loss: 123.593
iter:1600 | loss: 116.116
iter:1700 | loss: 98.273
iter:1800 | loss: 198.924
iter:1900 | loss: 83.108
iter:2000 | loss: 130.631
iter:2100 | loss: 178.169
iter:2200 | loss: 94.298
iter:2300 | loss: 177.642
iter:2400 | loss: 114.868
iter:2500 | loss: 224.557
iter:2600 | loss: 113.407
iter:2700 | loss: 171.517
iter:2800 | loss: 172.472
iter:2900 | loss: 90.006
iter:3000 | loss: 146.968
iter:3100 | loss: 121.504
iter:3200 | loss: 136.738
iter:3300 | loss: 104.490
iter:3400 | loss: 106.795
iter:3500 | loss: 156.526
iter:3600 | loss: 212.714
iter:3700 | loss: 132.980
iter:3800 | loss: 120.172
iter:3900 | loss: 106.936
| end of epoch  27 | time: 856.63s | train_loss 143.5837 | valid rse 0.0503 | valid rae 0.0351 | valid corr  0.9410
iter:  0 | loss: 131.354
iter:100 | loss: 151.220
iter:200 | loss: 124.472
iter:300 | loss: 164.179
iter:400 | loss: 119.283
iter:500 | loss: 136.749
iter:600 | loss: 178.487
iter:700 | loss: 114.396
iter:800 | loss: 144.599
iter:900 | loss: 160.741
iter:1000 | loss: 152.992
iter:1100 | loss: 122.666
iter:1200 | loss: 108.584
iter:1300 | loss: 140.079
iter:1400 | loss: 160.965
iter:1500 | loss: 165.386
iter:1600 | loss: 88.531
iter:1700 | loss: 161.939
iter:1800 | loss: 148.190
iter:1900 | loss: 134.175
iter:2000 | loss: 113.769
iter:2100 | loss: 132.434
iter:2200 | loss: 193.405
iter:2300 | loss: 156.598
iter:2400 | loss: 241.838
iter:2500 | loss: 354.309
iter:2600 | loss: 105.931
iter:2700 | loss: 129.928
iter:2800 | loss: 195.521
iter:2900 | loss: 135.432
iter:3000 | loss: 88.394
iter:3100 | loss: 190.548
iter:3200 | loss: 203.616
iter:3300 | loss: 98.958
iter:3400 | loss: 151.619
iter:3500 | loss: 118.762
iter:3600 | loss: 133.476
iter:3700 | loss: 118.699
iter:3800 | loss: 137.322
iter:3900 | loss: 122.069
| end of epoch  28 | time: 857.74s | train_loss 143.4706 | valid rse 0.0507 | valid rae 0.0354 | valid corr  0.9412
iter:  0 | loss: 188.864
iter:100 | loss: 129.606
iter:200 | loss: 189.086
iter:300 | loss: 98.505
iter:400 | loss: 102.962
iter:500 | loss: 81.402
iter:600 | loss: 210.443
iter:700 | loss: 171.606
iter:800 | loss: 112.966
iter:900 | loss: 135.003
iter:1000 | loss: 121.042
iter:1100 | loss: 126.450
iter:1200 | loss: 122.303
iter:1300 | loss: 144.517
iter:1400 | loss: 113.140
iter:1500 | loss: 88.098
iter:1600 | loss: 156.617
iter:1700 | loss: 117.363
iter:1800 | loss: 121.832
iter:1900 | loss: 189.603
iter:2000 | loss: 200.786
iter:2100 | loss: 200.652
iter:2200 | loss: 139.104
iter:2300 | loss: 107.930
iter:2400 | loss: 125.749
iter:2500 | loss: 106.945
iter:2600 | loss: 148.549
iter:2700 | loss: 232.076
iter:2800 | loss: 249.197
iter:2900 | loss: 130.016
iter:3000 | loss: 126.863
iter:3100 | loss: 111.908
iter:3200 | loss: 275.632
iter:3300 | loss: 158.787
iter:3400 | loss: 183.295
iter:3500 | loss: 84.943
iter:3600 | loss: 98.601
iter:3700 | loss: 128.212
iter:3800 | loss: 128.432
iter:3900 | loss: 112.351
| end of epoch  29 | time: 858.20s | train_loss 142.8840 | valid rse 0.0508 | valid rae 0.0351 | valid corr  0.9417
iter:  0 | loss: 201.479
iter:100 | loss: 99.952
iter:200 | loss: 177.879
iter:300 | loss: 152.744
iter:400 | loss: 99.848
iter:500 | loss: 134.978
iter:600 | loss: 107.736
iter:700 | loss: 102.009
iter:800 | loss: 138.851
iter:900 | loss: 199.147
iter:1000 | loss: 187.161
iter:1100 | loss: 119.265
iter:1200 | loss: 162.247
iter:1300 | loss: 118.916
iter:1400 | loss: 238.042
iter:1500 | loss: 196.712
iter:1600 | loss: 154.783
iter:1700 | loss: 225.800
iter:1800 | loss: 127.918
iter:1900 | loss: 129.882
iter:2000 | loss: 142.885
iter:2100 | loss: 288.588
iter:2200 | loss: 237.425
iter:2300 | loss: 90.131
iter:2400 | loss: 118.092
iter:2500 | loss: 155.540
iter:2600 | loss: 143.400
iter:2700 | loss: 138.533
iter:2800 | loss: 129.304
iter:2900 | loss: 155.602
iter:3000 | loss: 161.432
iter:3100 | loss: 165.290
iter:3200 | loss: 136.141
iter:3300 | loss: 122.037
iter:3400 | loss: 181.420
iter:3500 | loss: 193.925
iter:3600 | loss: 145.196
iter:3700 | loss: 230.743
iter:3800 | loss: 108.690
iter:3900 | loss: 220.670
| end of epoch  30 | time: 856.30s | train_loss 142.4801 | valid rse 0.0512 | valid rae 0.0376 | valid corr  0.9414
test rse 0.0763 | test rae 0.0449 | test corr 0.9479
final test rse 0.0763 | test rae 0.0426 | test corr 0.9452
Namespace(L1Loss=True, batch_size=4, buildA_true=True, clip=5, conv_channels=16, data='./data/electricity.txt', device='cuda:0', dilation_exponential=2, dropout=0.3, end_channels=64, epochs=30, gcn_depth=2, gcn_true=True, horizon=3, in_dim=1, layers=5, log_interval=2000, lr=0.0001, node_dim=40, normalize=2, num_nodes=321, num_split=1, optim='adam', propalpha=0.05, residual_channels=16, save='./models/model-electricity-3.pt', seq_in_len=168, seq_out_len=1, skip_channels=32, step_size=100, subgraph_size=20, tanhalpha=3, weight_decay=1e-05)
The recpetive field size is 187
Number of model parameters is 362385
begin training
iter:  0 | loss: 3913.759
iter:100 | loss: 327.338
iter:200 | loss: 476.575
iter:300 | loss: 478.792
iter:400 | loss: 321.476
iter:500 | loss: 254.096
iter:600 | loss: 391.523
iter:700 | loss: 227.609
iter:800 | loss: 272.172
iter:900 | loss: 140.251
iter:1000 | loss: 212.775
iter:1100 | loss: 206.175
iter:1200 | loss: 306.130
iter:1300 | loss: 199.989
iter:1400 | loss: 230.452
iter:1500 | loss: 227.699
iter:1600 | loss: 200.021
iter:1700 | loss: 248.206
iter:1800 | loss: 291.669
iter:1900 | loss: 170.122
iter:2000 | loss: 239.933
iter:2100 | loss: 191.168
iter:2200 | loss: 202.045
iter:2300 | loss: 190.577
iter:2400 | loss: 185.192
iter:2500 | loss: 238.624
iter:2600 | loss: 147.819
iter:2700 | loss: 190.532
iter:2800 | loss: 261.201
iter:2900 | loss: 218.239
iter:3000 | loss: 168.641
iter:3100 | loss: 218.454
iter:3200 | loss: 160.606
iter:3300 | loss: 247.878
iter:3400 | loss: 152.016
iter:3500 | loss: 192.464
iter:3600 | loss: 243.414
iter:3700 | loss: 285.546
iter:3800 | loss: 107.006
iter:3900 | loss: 181.873
| end of epoch   1 | time: 854.72s | train_loss 237.0179 | valid rse 0.0593 | valid rae 0.0427 | valid corr  0.9227
iter:  0 | loss: 131.749
iter:100 | loss: 284.778
iter:200 | loss: 151.864
iter:300 | loss: 115.103
iter:400 | loss: 136.381
iter:500 | loss: 246.836
iter:600 | loss: 148.473
iter:700 | loss: 132.410
iter:800 | loss: 266.430
iter:900 | loss: 160.526
iter:1000 | loss: 153.497
iter:1100 | loss: 166.102
iter:1200 | loss: 171.875
iter:1300 | loss: 136.934
iter:1400 | loss: 152.823
iter:1500 | loss: 162.023
iter:1600 | loss: 193.902
iter:1700 | loss: 226.593
iter:1800 | loss: 224.517
iter:1900 | loss: 183.866
iter:2000 | loss: 127.312
iter:2100 | loss: 127.629
iter:2200 | loss: 191.821
iter:2300 | loss: 131.510
iter:2400 | loss: 121.441
iter:2500 | loss: 156.477
iter:2600 | loss: 155.725
iter:2700 | loss: 226.372
iter:2800 | loss: 251.152
iter:2900 | loss: 165.928
iter:3000 | loss: 200.771
iter:3100 | loss: 97.423
iter:3200 | loss: 166.063
iter:3300 | loss: 141.578
iter:3400 | loss: 123.760
iter:3500 | loss: 147.278
iter:3600 | loss: 231.298
iter:3700 | loss: 294.298
iter:3800 | loss: 180.544
iter:3900 | loss: 175.760
| end of epoch   2 | time: 857.72s | train_loss 175.4470 | valid rse 0.0570 | valid rae 0.0417 | valid corr  0.9280
iter:  0 | loss: 133.356
iter:100 | loss: 139.443
iter:200 | loss: 185.777
iter:300 | loss: 157.533
iter:400 | loss: 162.196
iter:500 | loss: 228.247
iter:600 | loss: 126.238
iter:700 | loss: 142.315
iter:800 | loss: 136.354
iter:900 | loss: 237.920
iter:1000 | loss: 189.464
iter:1100 | loss: 182.408
iter:1200 | loss: 126.943
iter:1300 | loss: 200.682
iter:1400 | loss: 158.763
iter:1500 | loss: 176.473
iter:1600 | loss: 284.369
iter:1700 | loss: 182.034
iter:1800 | loss: 152.422
iter:1900 | loss: 133.013
iter:2000 | loss: 107.194
iter:2100 | loss: 183.043
iter:2200 | loss: 171.871
iter:2300 | loss: 130.774
iter:2400 | loss: 209.419
iter:2500 | loss: 163.996
iter:2600 | loss: 189.360
iter:2700 | loss: 180.207
iter:2800 | loss: 136.146
iter:2900 | loss: 166.054
iter:3000 | loss: 135.449
iter:3100 | loss: 153.203
iter:3200 | loss: 189.477
iter:3300 | loss: 154.313
iter:3400 | loss: 124.454
iter:3500 | loss: 161.059
iter:3600 | loss: 190.064
iter:3700 | loss: 170.035
iter:3800 | loss: 183.247
iter:3900 | loss: 121.883
| end of epoch   3 | time: 857.81s | train_loss 166.8148 | valid rse 0.0596 | valid rae 0.0450 | valid corr  0.9310
iter:  0 | loss: 125.284
iter:100 | loss: 324.122
iter:200 | loss: 119.302
iter:300 | loss: 178.700
iter:400 | loss: 98.258
iter:500 | loss: 170.398
iter:600 | loss: 180.083
iter:700 | loss: 125.631
iter:800 | loss: 141.012
iter:900 | loss: 119.783
iter:1000 | loss: 158.191
iter:1100 | loss: 159.708
iter:1200 | loss: 106.268
iter:1300 | loss: 150.574
iter:1400 | loss: 207.943
iter:1500 | loss: 172.643
iter:1600 | loss: 249.068
iter:1700 | loss: 120.606
iter:1800 | loss: 112.421
iter:1900 | loss: 180.291
iter:2000 | loss: 161.311
iter:2100 | loss: 106.137
iter:2200 | loss: 135.034
iter:2300 | loss: 134.963
iter:2400 | loss: 107.587
iter:2500 | loss: 197.838
iter:2600 | loss: 144.486
iter:2700 | loss: 250.662
iter:2800 | loss: 140.480
iter:2900 | loss: 163.794
iter:3000 | loss: 175.427
iter:3100 | loss: 150.910
iter:3200 | loss: 143.116
iter:3300 | loss: 204.333
iter:3400 | loss: 168.107
iter:3500 | loss: 167.127
iter:3600 | loss: 157.848
iter:3700 | loss: 178.472
iter:3800 | loss: 246.702
iter:3900 | loss: 112.226
| end of epoch   4 | time: 856.76s | train_loss 161.6778 | valid rse 0.0546 | valid rae 0.0388 | valid corr  0.9326
iter:  0 | loss: 160.640
iter:100 | loss: 168.710
iter:200 | loss: 103.404
iter:300 | loss: 182.299
iter:400 | loss: 116.410
iter:500 | loss: 131.149
iter:600 | loss: 284.407
iter:700 | loss: 202.352
iter:800 | loss: 203.897
iter:900 | loss: 172.115
iter:1000 | loss: 152.827
iter:1100 | loss: 164.602
iter:1200 | loss: 159.777
iter:1300 | loss: 160.389
iter:1400 | loss: 107.178
iter:1500 | loss: 191.525
iter:1600 | loss: 169.624
iter:1700 | loss: 145.112
iter:1800 | loss: 159.141
iter:1900 | loss: 150.962
iter:2000 | loss: 249.835
iter:2100 | loss: 137.377
iter:2200 | loss: 144.244
iter:2300 | loss: 128.481
iter:2400 | loss: 230.708
iter:2500 | loss: 175.246
iter:2600 | loss: 187.928
iter:2700 | loss: 129.619
iter:2800 | loss: 141.584
iter:2900 | loss: 143.712
iter:3000 | loss: 158.039
iter:3100 | loss: 112.621
iter:3200 | loss: 125.702
iter:3300 | loss: 141.313
iter:3400 | loss: 110.559
iter:3500 | loss: 168.097
iter:3600 | loss: 105.925
iter:3700 | loss: 125.953
iter:3800 | loss: 113.307
iter:3900 | loss: 120.602
| end of epoch   5 | time: 857.73s | train_loss 158.4110 | valid rse 0.0540 | valid rae 0.0382 | valid corr  0.9345
test rse 0.0797 | test rae 0.0451 | test corr 0.9419
iter:  0 | loss: 134.287
iter:100 | loss: 120.210
iter:200 | loss: 183.306
iter:300 | loss: 148.441
iter:400 | loss: 171.801
iter:500 | loss: 159.517
iter:600 | loss: 116.025
iter:700 | loss: 126.513
iter:800 | loss: 138.240
iter:900 | loss: 140.877
iter:1000 | loss: 201.387
iter:1100 | loss: 128.096
iter:1200 | loss: 194.939
iter:1300 | loss: 180.331
iter:1400 | loss: 143.305
iter:1500 | loss: 182.558
iter:1600 | loss: 147.704
iter:1700 | loss: 154.668
iter:1800 | loss: 197.049
iter:1900 | loss: 123.185
iter:2000 | loss: 169.751
iter:2100 | loss: 136.968
iter:2200 | loss: 110.814
iter:2300 | loss: 130.020
iter:2400 | loss: 100.469
iter:2500 | loss: 155.658
iter:2600 | loss: 117.747
iter:2700 | loss: 157.932
iter:2800 | loss: 137.546
iter:2900 | loss: 102.198
iter:3000 | loss: 143.121
iter:3100 | loss: 212.270
iter:3200 | loss: 187.227
iter:3300 | loss: 123.648
iter:3400 | loss: 206.918
iter:3500 | loss: 184.687
iter:3600 | loss: 145.338
iter:3700 | loss: 118.322
iter:3800 | loss: 126.519
iter:3900 | loss: 243.131
| end of epoch   6 | time: 856.34s | train_loss 156.3987 | valid rse 0.0525 | valid rae 0.0372 | valid corr  0.9353
iter:  0 | loss: 138.588
iter:100 | loss: 124.783
iter:200 | loss: 150.900
iter:300 | loss: 98.246
iter:400 | loss: 178.365
iter:500 | loss: 131.869
iter:600 | loss: 165.091
iter:700 | loss: 149.128
iter:800 | loss: 152.654
iter:900 | loss: 194.615
iter:1000 | loss: 102.144
iter:1100 | loss: 117.512
iter:1200 | loss: 160.046
iter:1300 | loss: 222.165
iter:1400 | loss: 144.240
iter:1500 | loss: 126.947
iter:1600 | loss: 212.792
iter:1700 | loss: 170.478
iter:1800 | loss: 165.841
iter:1900 | loss: 114.456
iter:2000 | loss: 188.824
iter:2100 | loss: 184.854
iter:2200 | loss: 131.721
iter:2300 | loss: 121.615
iter:2400 | loss: 90.047
iter:2500 | loss: 278.137
iter:2600 | loss: 99.471
iter:2700 | loss: 109.559
iter:2800 | loss: 142.992
iter:2900 | loss: 175.879
iter:3000 | loss: 97.813
iter:3100 | loss: 197.997
iter:3200 | loss: 185.806
iter:3300 | loss: 171.800
iter:3400 | loss: 207.849
iter:3500 | loss: 163.335
iter:3600 | loss: 130.899
iter:3700 | loss: 226.160
iter:3800 | loss: 150.697
iter:3900 | loss: 157.178
| end of epoch   7 | time: 857.25s | train_loss 154.7392 | valid rse 0.0514 | valid rae 0.0359 | valid corr  0.9363
iter:  0 | loss: 184.739
iter:100 | loss: 140.209
iter:200 | loss: 146.687
iter:300 | loss: 178.634
iter:400 | loss: 109.070
iter:500 | loss: 162.097
iter:600 | loss: 227.909
iter:700 | loss: 246.294
iter:800 | loss: 136.810
iter:900 | loss: 160.085
iter:1000 | loss: 147.185
iter:1100 | loss: 139.970
iter:1200 | loss: 122.088
iter:1300 | loss: 127.915
iter:1400 | loss: 175.490
iter:1500 | loss: 156.899
iter:1600 | loss: 136.806
iter:1700 | loss: 154.884
iter:1800 | loss: 124.490
iter:1900 | loss: 120.679
iter:2000 | loss: 141.265
iter:2100 | loss: 155.074
iter:2200 | loss: 123.848
iter:2300 | loss: 152.723
iter:2400 | loss: 191.902
iter:2500 | loss: 177.252
iter:2600 | loss: 174.596
iter:2700 | loss: 141.112
iter:2800 | loss: 128.338
iter:2900 | loss: 127.153
iter:3000 | loss: 110.744
iter:3100 | loss: 90.219
iter:3200 | loss: 158.557
iter:3300 | loss: 94.519
iter:3400 | loss: 171.860
iter:3500 | loss: 193.620
iter:3600 | loss: 101.489
iter:3700 | loss: 137.854
iter:3800 | loss: 113.876
iter:3900 | loss: 230.897
| end of epoch   8 | time: 857.89s | train_loss 153.4789 | valid rse 0.0507 | valid rae 0.0359 | valid corr  0.9344
